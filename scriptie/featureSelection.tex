\chapter{Methods}
{\samenvatting This chapter starts by explaining well-known features for emotion recognition. Then an overview of some emotion recognition studies is given. The chapter ends by going through frequently used feature selection methods.}

\section{Features}
\label{featuresExplained}
Usually, good features are needed to train a machine learning algorithm. In the context of this thesis, good features should be correlated with the emotion. Two categories of features are observed: EEG features and non-EEG features. Both categories are covered in the following sections.

\subsection{EEG-features}
EEG features are extracted from the electroencephalography measurements from the subject's scalp. The power spectral density (PSD) \nomenclature{PSD}{Power Spectral Density} of a signal gives the distribution of the signal's energy in the frequency domain. By calculating the spectral density for different wavebands of the signal, one can determine how much alpha, beta, ... power is in the signal.

\npar

Differential entropy (DE)\nomenclature{DE}{Differential Entropy} is defined as follows \citep{killyPaper} \\
\begin{center}
$ - \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi\sigma^2}} exp(\frac{(x-\mu)^2}{2\sigma^2}) log(\frac{1}{\sqrt{2\pi\sigma^2}}) exp(\frac{(x-\mu)^2}{2\sigma^2})dx$
\end{center}
According to \citep{diffEnt}, the differential entropy of a certain band is equivalent to the logarithmic power spectral density for a fixed length EEG sequence, which simplifies the calculations significantly.
\begin{center}
$DE_{channel} = log(PSD_{channel})$
\end{center}

\npar

The most known feature for valence recognition is the frontal asymmetry of the alpha power\cite{GivenPaper}. The right hemisphere is generally speaking, more active during negative emotion than the left hemisphere which is in turn more active during positive emotions\cite{RealTimeEEGEmotion,EEGDatasets,killyPaper}. The asymmetry can be calculated in different ways. First, one can calculate the differential asymmetry (DASM) \nomenclature{DASM}{Differential Asymmetry}, where the left alpha power is subtracted from the right alpha power.

\begin{center}
$DASM = DE_{left} - DE_{right}$
\end{center}

Another way to measure the asymmetry is by division. The Rational Asymmetry (RASM) \nomenclature{RASM}{Rational Asymmetry} does exactly this and is given by: \\

\begin{center}
$RASM = \frac{DE_{left}}{DE_{right}}$
\end{center}

With $DE_{left}$ and $DE_{right}$ being the left and right differential entropy respectively.

\npar

Another reported feature in literature is the caudality, or the asymmetry in fronto-posterior direction\cite{caudality}, meaning the difference in power between the front and the back of the scalp. This can again be calculated in two ways. The first method is the differential Caudality (DCAU) \nomenclature{DCAU}{Differential Caudality} is defined as: \\

\begin{center}
$DCAU = DE_{front} - DE_{post}$
\end{center}

Another way to determine the Caudality is the Rational Caudality (RCAU) \nomenclature{RCAU}{Rational Caudality}, which is defined as:

\begin{center}
$RCAU = \frac{DE_{front}}{DE_{post}}$
\end{center}

With $DE_{front}$ and $DE_{post}$ being the frontal and posterior power respectively.

\npar

Arousal is usually determined, by looking at the different wavebands\citep{ExtendedPaper}. Each waveband has their own medical interpretation, see \ref{wavebands}. Alpha power corresponds to a more relaxed brain, while Beta power corresponds to a more active brain. The alpha / beta ratio therefore seems a good indicator for the arousal state of a person.

\npar

The Alpha/ Beta ratio is limited to comparing two wavebands. Other frequently used features are fractions of PSD. Where the fractions of waveband power is determined for a channel, given by:

\begin{center}
$frac_{band} = \frac{power_{band}}{power_{total}}$
\end{center}

The fractions give insight in the distributions of wavebands at different channel locations.

\subsection{non-EEG features}
The aforementioned EEG features are just one class of physiological features. The DEAP dataset contains several physiological measurements, listed below \citep{DEAP}. For each of these measurements the average, standard deviation, variation, median, minimum, maximum and the standard deviation is calculated.

\npar

The Galvanic Skin Response \nomenclature{GSR}{Galvanic Skin Response} uses two electrodes on the middle and index finger of the subjects left hand to measure the skin resistance. It has been reported that the mean value of the GSR is related to the level of arousal\citep{GSR, DEAP}.

\npar

The respiration belt, indices the user's respiration rate. Slow respiration is linked to relaxation (low arousal), while fast and irregular respiration patterns corresponds to anger or fear, both emotions with low valence and high arousal\citep{DEAP}.

\npar

A plethysmograph is a measurement of the volume of blood in the subject's left thumb. This can be interpreted as the the blood pressure. Blood pressure offers valuable insight into the emotional state of a person as it correlates with emotion; stress is known to increase blood pressure\citep{DEAP}.

\npar

The heart rate is not directly in the DEAP dataset, but can be extracted from the plethysmograph, by looking at local minima and maxima\citep{DEAP}. This is clearly visible when looking at the plethysmograph's output, shown in Figure \ref{before}.

\mijnfiguur{width=0.7\textwidth}{before}{The plethysmograph before smoothing.}

The heart rate extraction is done in two steps. First the plethysmograph signal is smoothed, by filtering out the high frequency components to avoid noise being selected as a local optima. In the second stage the local optima are located, this is shown in Figure \ref{extrema}

\mijnfiguur{width=0.7\textwidth}{extrema}{The local optima in the plethysmograph.}

\npar

The combination of a local minima and maxima correspond to a heart beat, therefore the time between two consecutive local minima or maxima correspond to the time between two heart beats, known as the interbeat interval. Getting the average heart rate from the interbeat interval is straight forward. Lastly, the skin temperature of the subject is also available.

\section{Emotion recognition Studies}
This section will give a overview of other relevant studies that did similar research combined with their results and conclusions. Some of these studies also did some research on cross-subject emotion recognition, but as the research for emotion recognition is still in its infancy\citep{emorecoghard} and subject independent features are hard to find \citep{DEAP}, the research is more focusses on person specific emotion recognition.

\subsection{DEAP method}
The first method of emotion recognition is the DEAP method, described in the DEAP paper\citep{DEAP}. The research found that Valence shows the strongest correlations with the EEG signals. Additionally the study found correlations in all frequency bands, with an increase in power for the lower range wavebands for an increase in valence. These effects occur in the occipital regions of the brain, above the visual cortices, which might indicate that the subject is focussing on a pleasurable sound. A central decrease in beta power was observed together with a occipital and right temporal increase in power for positive emotions. The research conclude that these observed correlations concur with other neurological studies, but that the absolute value of the correlations are seldom bigger than $0.1$ for a cross person setting, which indicates that cross person emotion recognition is a non trivial problem. The absolute values of the person specific correlations were around $0.5$.

\npar

The DEAP paper also presents their own classification method for person specific emotion classification. They start by performing feature selection using the Fisher's linear discriminant for feature selection. The Fisher's linear discriminant is defined as:

\begin{center}
$J(f) = \frac{|\mu_1 - \mu_2|}{\sigma_1^2 + \sigma_2^2}$ \
\end{center}

With $\mu$ and $\sigma$ being the mean and standard deviation of feature f. The Fisher's discriminant was calculated for each feature, before a threshold of 0.3 was applied to filter out irrelevant features. The used classifier was a Naive Bayes classifier, which assumes independence of features. The Naive Bayes classifier is a simple classifier that uses the following equation:

\begin{center}
$G(f_1, ..., f_n) = argmax_c p(C=c) \prod\limits_{i=1}^n p(F_i=f_i|C=c)$ \\
\end{center}

With F being the set of features and C the classes. $p(F_i=f_i|C=c)$ is estimated by assuming Gaussian distributions of features and modelling these from the training set.

\subsection{Stable Emotion Recognition over Time}

In \citep{killyPaper}, research is done to find EEG patterns for emotion recognition that are stable over time. EEG patterns are not only subject dependent, they are also dependent on the subjects mood and thus might vary in time. The paper starts by researching different EEG features: PSD, DE, DASM, RASM, DCAU, RCAU, these features are explained in \ref{featuresExplained} and are tested on the DEAP dataset. Afterwards, they develop a new dataset, where subjects have several trials with some time in between.

\npar

Their machine learning set-up is as follows, first they perform feature extraction of the aforementioned features. Then feature smoothing is done using a Linear Dynamic system (LDS) \nomenclature{LDS}{Linear Dynamic System}, that can be expressed by:
\begin{center}
$x_t = z_t + w_t$\\
$z_t = Az_{t-1} + v_t$
\end{center}
$x_t$ denotes the observed variables or features, while $z_t$ denotes the hidden emotion variables. $A$ is a transformation matrix and $w_t$ is Gaussian noise. The need for a linear dynamic system is supported by the assumption that emotion change gradually over time. The LDS filters out components that are not associated with emotional states.

\npar

The list of features at this point is too big and may contain uncorrelated features that might lead to performance degradation of the classifier. Two methods for this are compared, principal component analysis (PCA) and minimal redundancy maximal relevance (MRMR)\nomenclature{MRMR}{Minimal Redundancy Maximal Relevance}. 

\npar

PCA uses an orthogonal transformation to create a lower dimensional feature space starting from the original higher dimensional feature space. It does so by minimizing the loss of information, i.e. the principal component should have the largest possible variance. 

\npar

PCA cannot preserve original domain information like channel and frequency, therefore the paper also uses the MRMR method. MRMR uses mutual information in combination with maximal dependency criterion and minimal redundancy. The algorithm starts by searching features satisfying:

\begin{center}
$max D(S,c), D=\frac{1}{|S|} {\displaystyle \sum_{x_d \in S}} I(x_d;c)$
\end{center}

Where S is the feature subset to select. When two features are highly correlated, the maximal dependency is not likely to change when one of the correlated features is removed. This is expressed by the minimal redundancy condition.

\begin{center}
$min R(S), R = \frac{1}{|S|^2} {\displaystyle \sum_{x_{di}, x_{dj} \in S}} I(x_{di},x_{dj})$
\end{center}

The two conditions are then combined to from the Maximal Relevance Minimum Redundancy, which can be expressed as:

\begin{center}
$max \varphi(D,R), \varphi=D-R$
\end{center}
Note that incremental search methods exists and are often used in practice. After performing the dimensionality reduction, the samples from the DEAP data set are classified in high / low valence and high/low arousal, giving a total of four classes. All values close to the separation border are removed from the training data, as they might confuse the classifier. 

\npar 
For the classification, three conventional and one newly developed pattern classifiers were compared. k-nearest neighbors (KNN) \nomenclature{KNN}{k-nearest neightbors}, logistic regression (LR)\nomenclature{LR}{Logistic Regression}, Support Vector Machines (SVM) and Graph regularized Extreme Learning Machine (GELM) \nomenclature{GELM}{Graph regularized Extreme Learning Machine}. 

\npar

Extreme Learning Machine (ELM) \nomenclature{ELM}{Extreme Learning Machine} is a single layer feed forward neural network\citep{ELMpaper}. GELM is based on the idea that similar shapes should have similar properties and obtains better results for face recognition\citep{GELMpaper} and as the paper concludes, also for emotion classification.

\npar

The study found then performed a study on the different features and concluded that DE features are the most suitable EEG features, followed by the asymmetry features (RASM, DASM, DCAU and RCAU). The LDS smoothing was also found to be the better feature smoothing method. 

\subsection{EEG-based emotion recognition in music listening}

This study\citep{emorecoghard} uses EEG features to recognize 4 different emotions (joy, anger, sadness, pleasure) that were induced by music. They compared four different feature sets on 6 different wavebands: RASM and DASM of 12 channelpairs, raw PSD of the 24 channels and PSD of 30 channels (including 6 midline channels). The compared set of wavebands consists of: alpha, beta, gamma, delta, tetha and all wavebands. These features were fed to two different classifiers, one Multilayer perceptron (MLP) \nomenclature{MLP}{Multilayer Perceptron} and an SVM. 

\npar

Their main results were that the DASM features worked better that the RASM features and even better than using the corresponding PSD features. They also did research to person independent EEG features and found that their accuracy remained consistent. Note that while these results sound promising, they were unfortunately not performed on the DEAP dataset.  

\section{From physiological signals to emotion: implementing and comparing selected methods for feature extraction and classification}

Physiological signals are less used for emotion recognition, but offer the advantage that are more robust against social masking as the physiological signals are controlled by the human autonomous nervous system\citep{PhytoEm}. the disadvantage is that physiological signals suffer from motion artefacts and the mapping of physiological patterns to emotion is a non trivial problem. Automatically selecting the most significant features and fine tuning the classifiers to a specific person is therefore desired.

\npar

This paper classifier four distinct emotions (joy, anger, sadness and pleasure) triggered by songs that remind the subject of memories of the corresponding emotions. The four emotions are mapped in the valence-arousal model. The used features were typical statistical values of physiological signals (Skin Conductivity (SC), Electrocardiogram (ECG), Electromygraphy (EMG) and Respiration rate. They compared several techniques: Analysis of Variance (ANOVA) \nomenclature{ANOVA}{Analysis of Variance} where the best d features were taken. Sequential forward selection (SFS) \nomenclature{SFS}{Sequential Forward Selection}, where the algorithm starts with an empty feature set and then introduces a new feature in each iteration, and sequential backward selection (SBS) \nomenclature{SBS}{Sequential Backward Selection} where a feature is removed in each iteration, were also tested. They compared the feature selection to two dimensionality reduction methods: PCA and Fisher projection. The difference between dimensionality reduction and feature selection is that dimensionality reduction methods consider all information in the feature space, whereas feature selection methods take a subset of the information.

\npar

The newly formed feature space was then fed to three different classifiers: K-nearest neighbors, Multilayer perceptron and Linear discriminant function. The results indicated that is was easier to classify arousal than valence. Non-eeg feature thus seem good features for arousal classification. SFS in combination with fisher seemed to give the best classification performance, closely followed by LDF and ANOVA.

\npar

The paper also concludes that joy was characterized by a faster heart rate, while sadness was identified by low SC and EMG signals. There was also a higher breathing rate for negative valence emotions. They reported limited similarities for the selected features between subjects.

\subsection{Research component in this thesis}
%different with this thesis

It is clear that from the aforementioned papers, some research has already been done for emotion recognition. The contribution of this thesis is threefold, first compare a bigger ranger of feature selection methods on a bigger set of physiological features. Second, compare EEG features to non-EEG features to see how much information can be retrieved from the EEG signals compared to the non-EEG signals, which are usually easier to obtain. Third, perform the feature selection methods in a cross-subject setting to see which feature generalise well across subjects. Last the feature selection will be determined for the DEAP dataset, so that it can server as a benchmark. Emotion recognition studies based on physiological signals often depend for a large proportion on the used dataset \citep{PhytoEm}.

\section{Feature selection methods}
Feature selection is the process of selecting good features from a set of features. The need for this is twofold: first reducing the number of features, is a protection mechanism against overfitting. This is important when a smaller dataset is used. Second, reducing the number of features can speedup the learning process of a learning algorithm as fewer parameters need to be optimized. Additionally, in the context of research, looking at which features are important might give more insight in how emotion is processed by the brain, i.e. knowing what feature are relevant can help neuroscientists understand the working of the brain better.

\subsection{Independent Metrics}
These feature selection methods select features based on statistical tests or another machine-learning independent metric. 

\subsubsection{Pearson Correlation}
The Pearson correlation coefficient measures the linear relationship between two variables. The output is a value r, that lies between -1 and 1, corresponding to perfect negative correlation and perfect positive correlation respectively. A correlation value of 0 means that there is no correlation.

\npar

More formally\citep{corrPaper}, the Pearson product-moment coefficient of correlation, r between variables $X_i$ and $Y_i$ of datasets $X$ and $Y$ is defined as:


\begin{center}
$r = \frac{SS_{xy}}{\sqrt{SS_{xx}SS_{yy}}}$
\end{center}
with
\begin{center}
$SS_{xy} = \sum\limits_i (X_i-\tilde{X})(Y_i-\tilde{Y})$
\end{center}
and
\begin{center}
$SS_{xx} = \sum\limits_i (X_i-\tilde{X})^2$ \\
$SS_{yy} = \sum\limits_i (y_i-\tilde{Y})^2$
\end{center}

\npar

The Pearson correlation coefficient is fast and simple to calculate, but has some major shortcomings. First off, it can only see linear relation ships and will not see the correlation between a value $x$ and $x^2$.

\npar

In the context of this thesis, whether the correlation is positive or negative is not important; a learning algorithm needs features that have significant correlation. As a result the absolute value of the r value is reported as this allows for a more convenient comparison of correlations.

\subsubsection{Mutual Information}
Mutual information is a more robust option for correlation estimation. The mutual information, I, of two variables $X$ and $Y$ is defined as \citep{mutPaper}:
\begin{center}
$I(X,Y) = \sum\limits_{y\in Y} \sum\limits_{x\in X} p(x,y)log(\frac{p(x,y)}{^(x)p(y)}$
\end{center}

\npar

Using the mutual information directly for feature ranking might be inconvenient for two reasons. Firstly, it doesn't lie in a fixed range and it is hard to compute for continuous variables. One solution for this problem is to normalize the mutual information scores, so that the results lies between 0 and 1.

The normalized mutual information, NMI of variables X and Y is given by:
\begin{center}
$NMI(X,Y) = \frac{I(X,Y)}{\sqrt{(H(X)H(Y))}}$
\end{center}
With $H(X)$ and $H(Y)$ being the Shannon entropy of variable X and variable Y, defined as:
\begin{center}
$H(X) = \sum\limits_{i\in X} p_ilog(\frac{1}{p_i}) = - \sum\limits_i p_ilog(p_i)$\\
$H(Y) = \sum\limits_{i\in Y} p_ilog(\frac{1}{p_i}) = - \sum\limits_i p_ilog(p_i)$
\end{center}
\npar

\subsubsection{Distance Correlation}
The Pearson correlation coefficient might give a correlation of zero for dependent variables, as shown in Figure \ref{pearson}.

\mijnfiguur{width=0.7\textwidth}{pearson}{Pearson correlation coefficients for different sets of (x,y) points. Note that many coefficients are zero, while there clearly is some correlation. Source: Wikipedia}

The distance covariance, sometimes referred as the Brownian covariance, addresses this problem\citep{distPaper}. Its main idea is that a good measurement for dependence is the 'distance' between the join distribution $f_{XY}$ and the product of the marginal distributions $f_X$ and $f_Y$ weighted by a weight function $W$. This gives the following theoretical function:

\begin{center}
$|| f_{XY} - f_Xf_y||_W$
\end{center}

The result is that the distance correlation metric gives very different results, as you can see when comparing the distance correlation outputs in Figure\ref{distcorr} with the pearson correlation outputs in Figure \ref{pearson}.

\mijnfiguur{width=0.7\textwidth}{distcorr}{Distance correlation coefficients for different sets of (x,y) points. Note the difference with the pearson correlation coefficients in Figure \ref{pearson}. Source: Wikipedia}

Without going further into the theory, the distance correlation between two variables X and Y, each with n data points can be calculated as follows.
First compute all pairwise euclidean distances for both variables.
\begin{center}
$[D_x]_{j,k} = || X_j - X_k||   j,k = 1,2,...,n$\\
$[D_Y]_{j,k} = || Y_j - Y_k||   j,k = 1,2,...,n$\\
\end{center}
The result is two n by n distance matrices a and b. Next center both matrices.
\begin{center}
$S_x = C_nD_xC_n$\\
$S_y = C_nD_yC_n$\\
\end{center}
Finally, compute the covariance.
\begin{center}
$\nu^2(X,Y) = \frac{1}{n^2} \sum\limits_l \sum\limits_k [S_x]_{k,l}[S_y]_{k,l}$ 
\end{center}

This metric is a covariance metric, which means that it is not normalized. The distance correlation is the normalized version of the distance covariance and is defined as:

\begin{center}
$dCor(X,Y) = \frac{dCov(X,Y)}{\sqrt{dVar(X)dVar(Y)}}$
\end{center}
With $dCov(X,Y)$ being the aforementioned distance covariance, $dVar(X)$ and $dVar(Y)$ are the distance standard deviations. The distance correlation has the disadvantage that is much slower than mutual information or Pearson correlation, but in return, the distance correlation is able to detect more complex relationships between two variables.

\subsubsection{Analysis of Variance}

Analysis of variance (ANOVA) \nomenclature{ANOVA}{Analysis of Variance} is a statistical test to analyse differences between groups. The idea is that the total variance, found in the samples consists of two parts. The first part is the variance within a single group, the second part is the variance between groups. 

\npar

Suppose you want to test the influence of caffeine on the reaction speed\footnote{This example was based on the following video: https://www.youtube.com/watch?v=ITf4vHhyGpc}. To do so, you take two groups of each 10 persons. The first group has to drink a large coffee, the second group is the control group that only drinks water, before the reaction time of all persons in the group are measured. Next the total variance can be calculated as well as the variance within each group and the variance between the groups. If the variance within each group is much larger than the variance in between the groups, one concludes that the groups are similar. The reaction time is thus dependent on the person and not on the caffeine. However should the variance between the groups be much bigger than the variance within each group, than one concludes that the variance in reaction time is caused by the caffeine. 



\subsection{Machine Learning Methods}
These methods select features by applying an arbitrary machine learning technique and looking at the coefficients of the features. Features with high coefficients have more influence on the end results than features with a lower coefficients. Note that again, the absolute value is of importance.

\subsubsection{Linear Regression}

A first method is simple linear regression, where a linear combination of features is searched that produces a good estimate of the output value. Linear regression can achieve good results given that the data doesn't contain a lot of noise and the features are (relatively) independent. When the set of features contains correlated features, the model becomes unstable. As a result, small changes in input data might lead to huge differences in output coefficients. for example assume the 'real output' is given by $Y = X_1 + X_2$ and the dataset contains output in the form of $Y = X_1 + X_2 + \epsilon$ with $\epsilon$ being some random noise. Further more assume that $X_1$ and $X_2$ are linearly correlated, meaning that $X_1 \approx X_2$. The suspected output of the model should be $Y = X_1 + X_2$, but since noise is added the algorithm might end up with arbitrary combinations of $X_1$ and $X_2$, e.g. $Y = -X_1 + 3X_2$ and rate one feature much higher than another one, while in reality they are of equal importance. This is due to the noise; by maximizing the performance, the algorithm will minimize the influence of noise on the output, which result in unstable behaviour when sufficient correlated features are present. 

\subsubsection{Lasso Regression}
Lasso regression uses L1 regularization, that adds a penalty $\alpha\sum\limits_{i=1}^{n} |w_i|$ to the loss function. the result is that the coefficients of weak features are forced to zero, as each non-zero feature adds to the penalty. This form is regularization is thus quite aggressive, it removes weak features completely. The problem with this is, again, stability; coefficient can vary significantly even for small changes in training data, when there are correlated features.

\subsubsection{Ridge Regression}
Ridge regression uses L2 regularization, which add a L2 norm penalty to the loss function, given by $\alpha\sum\limits_{i=1}^{n} w_i^2$. Where the L1 norm forces the coefficients to zero, the L2 regularization forces the coefficients to be spread out more equally. The result is that correlated features tend to get similar coefficients, as this minimizes the loss function, which in turn results in a more stable model. 

\subsubsection{SVM}

A Support vector machine (SVM) is a well known and proven method for machine learning. It is used in several Emotion recognition studies %TODO refs

An SVM works in essence by creating a hyperplane that separates two classes. Shown in Figure \ref{SVM1} is a simple line separating the red from the blue balls.

\mijnfiguur{width=0.5\textwidth}{SVM1}{One possible separation border.}

This is one possible solution, note that the best possible line is a line that maximizes the boundary between the two classes, shown in Figure \ref{SVM2}.

\mijnfiguur{width=0.5\textwidth}{SVM2}{A separation with maximal boundary.}

This all works well, as the balls are still separable using a single straight line. This is not always the case though; shown in Figure \ref{SVM3} is a scenario where it is not possible to separate the red balls from the blue ones.

\mijnfiguur{width=0.5\textwidth}{SVM3}{There exists no possible line that can separate the red balls from the blue ones.} 

A solution for this is to transform the features space, to transformed space, where it is possible to separate the balls using a hyperplane, this is shown in Figure \ref{SVM4}.

\mijnfiguur{width=0.7\textwidth}{SVM4}{Transformation to a new features space where the balls can be separated by a hyperplane.} 

Back in the original feature space the separation boundary might look like Figure \ref{SVM5}.

\mijnfiguur{width=0.5\textwidth}{SVM5}{Separation boundary in the original feature space.} 

\subsubsection{Random Forests}
Random forests (RF) \nomenclature{RF}{Random Forests} is a efficient learning algorithm based on model bagging and aggregation ideas\citep{rfPaper}. The Random forests work by creating different decision trees. On their own, decision trees are very prone to overfitting. Random forests solve this problem by creating an aggregation of trees. 

\npar

The word random in random forest indicates that some randomness is included. Each tree in a random forest looks at a random subset of the samples and a random subset of the features. This principle is shown in Figure \ref{RF}. This random subset of samples is called the bootstrap sample and is selected out of N samples, by picking N samples with replacement. This results, on average, in 2/3 of the samples being selected (with some doubles). The other 1/3 of the samples are then used as out of bag (oob) \nomenclature{OOB}{Out of Bag} set. Averaging the performance of each tree on the out of bag set, offers an indication of the generalisation of the random forest.

\mijnfiguur{width=0.9\textwidth}{RF}{The structure of a random forest, found at \citep{rfPic}}

\npar

To understand which features are good, one needs to understand the internal workings of a decision tree. Suppose the following example\footnote{This example is based extensively on this youtube video: https://www.youtube.com/watch?v=eKD5gxPPeY0}, where one tries to find an algorithm to predicted whether or not a person will play tennis on a given day. Suppose the training data is given by Table \ref{decisionTreeTable} and a prediction for the $15^{th}$ sample needs to be made.

\begin{table}[H]
\centering
\caption{suppose the following training examples for a decision tree.}
\label{decisionTreeTable}
\begin{tabular}{lllll}
\textbf{Day} & \textbf{Outlook} & \textbf{Humidity} & \textbf{Wind} & \textbf{Play tennis} \\
\textbf{1}   & sunny            & high              & weak          & no                   \\
\textbf{2}   & sunny            & high              & strong        & no                   \\
\textbf{3}   & overcast         & high              & weak          & yes                  \\
\textbf{4}   & rain             & high              & weak          & yes                  \\
\textbf{5}   & rain             & normal            & weak          & yes                  \\
\textbf{6}   & rain             & normal            & strong        & no                   \\
\textbf{7}   & overcast         & normal            & strong        & yes                  \\
\textbf{8}   & sunny            & high              & weak          & no                   \\
\textbf{9}   & sunny            & normal            & weak          & yes                  \\
\textbf{10}  & rain             & normal            & weak          & yes                  \\
\textbf{11}  & sunny            & normal            & strong        & yes                  \\
\textbf{12}  & overcast         & high              & strong        & yes                  \\
\textbf{13}  & overcast         & normal            & weak          & yes                  \\
\textbf{14}  & rain             & high              & strong        & no                   \\
             &                  &                   &               &                      \\
\textbf{15}  & rain             & high              & weak          & ?                   
\end{tabular}
\end{table}

\npar
A decision tree will take a feature and split the data based on the possible outcomes of this feature. In case the features are continuous values, ranges are selected. In some cases the leafs will be pure, like the leaves displayed in  green in Figure \ref{decisionTree}. All examples in here have the same output. In case the leave is not pure, then another split is needed. Note that not all random forests split until all leaves are pure; random forest can be limited in depth, in that case the output is chose by a majority voting of the samples.

\mijnfiguur{width=0.6\textwidth}{decisionTree}{A decision tree for the data in Table \ref{decisionTreeTable}}

Once the tree is constructed it becomes clear that the predicted output of sample 15 is yes. This is obtained simply by following the tree branches. Even though the features are selected at random, they have influence on the accuracy. Good features will reduce the impurity significantly, thus the impurity reductions are a good indication for how important a feature is.

\npar

Since the importance is averaged over different nodes and different trees, it is also capable of detecting combinations of features that work well. One feature may not be important on its own, but might be a very good feature when combined with other features. Suppose the following example in Table \ref{featPair}:

\begin{table}[H]
\centering
\begin{tabular}{lll}
\textbf{label} & \textbf{feature A} & \textbf{feature B} \\
\textbf{Happy} & +                  & +                  \\
\textbf{Happy} & -                  & -                  \\
\textbf{Sad}   & -                  & +                  \\
\textbf{Sad}   & +                  & -                 
\end{tabular}
\caption{Some feature are not significant on its own, but a might be part of a combination of features.\label{featPair}}
\end{table}

It is clear that feature A and B are very important when it comes to predicting whether or not a person is happy or sad. When both features have the same sign, the person is happy, otherwise he is not. This problem occurs in many simple selection methods. 

\npar

This problem does not occur for random forest though, as combinations of features are also 'tested' in the sense that a tree might split on them in different stages. Once the combination of features occurs randomly in a decision tree, the impurity will drop significantly, which will result in higher importance rankings.

\subsection{Dimensionality Reduction methods}
The algorithms described below perform a dimensionality reduction, often by projecting a high dimensional to a lower dimensional features space. Looking at coefficients of these trained models, gives insight in which features are important. Note that some of this methods could also be seem as machine learning algorithms. 

\subsubsection{Common Spatial Patterns}
Common Spatial Patterns (CSP)\nomenclature{CSP}{Common Spatial Patterns} is a supervised technique that has its origin in the optimization of motor imagery BCIs\citep{CSPSeba}. It is a common technique in BCI research\cite{ErrorPotentials,svmldacomp,currTrends}. CSP creates linear combinations of the original EEG channels that maximize the variance for one class while simultaneously minimizing the variance of the other class \cite{ErrorPotentials}. One disadvantage of using CSP is that the default version can only distinguish between 2 classes, though one can easily aggregate multiple CSP models to create one-vs-one and one-vs-all models, similarly to the one-vs-one and one-vs-all SVMs.

\npar

The input for a CSP filter is a set of N labelled samples $E_j (j=1...N)$, with dimension $N_{ch}$ x $T_j$, with $N_{ch}$ being the number of EEG channels and $T_j$ the number of samples in a single trial\citep{CSPSeba}.

\npar

First the train data is split into two classes, before computing the covariance matrices of both classes.
\begin{center}
$\Sigma_1 = {\displaystyle \sum_{j \in C_1}} X\frac{E_jE_j^T}{trace(E_jE_j^T)}$ \\
$\Sigma_2 = {\displaystyle \sum_{j \in C_2}} X\frac{E_jE_j^T}{trace(E_jE_j^T)}$ \\
\end{center}
Note that the average of $E_j$ is expected to be zero, because a bandpass filter is applied that make the DC component of the signal zero. The next step is to calculate the composite covariance matrix.
\begin{center}
$\Sigma = \Sigma_1 + \Sigma2$
\end{center}

\npar

Next the covariance matrix is diagonalised by calculating the eigenvalues and eigenvectors of $\Sigma$.
\begin{center}
$V^T\Sigma V = P$
\end{center}
The eigenvalues are then found on the diagonal of P, each eigenvalue corresponds to an eigenvector found in the columns of V.

\npar

The next step is the whitening transformation.
\begin{center}
$U = P^{\frac{1}{2}}V^T$ \\
\end{center}
Which results in
\begin{center}
$U\Sigma U^T = 1$
\end{center}
Next the following two matrices are calculated:
\begin{center}
$R_1 = U\Sigma_1U^T$\\
$R_2 = U\Sigma_2U^T$
\end{center}
$R_1$ is then diagonalised
\begin{center}
$Z^TR_1Z = D = diag(d1, ..., d_m)$
\end{center}
The eigenvalues on the diagonal are then sorted, as larger eigenvalues correspond to higher importances. %TODO
Next the filters are determined by:
\begin{center}
$W = Z^TU$
\end{center}
The EEG channels can then be filtered as follows:
\begin{center}
$E^{CSP} = WE^{orig}$
\end{center}

\npar

Since CSP filters create simple linear combination of incoming channels, they can also be used as feature selection mechanism,albeit in a limited fashion. The result of a CSP transformation are again a set of eeg channels, where each channel is a combination of the previous channels. The first and last row of the resulting matrix $W$ shows the coefficients for which the variance is maximized between the two signals. Looking at those coefficients, one can determine which channels are of more importance than other and thus which channel locations have the most influence on emotion.

\subsubsection{Linear Discriminant Analysis}
Linear Discriminant Analysis (LDA)\nomenclature{LDA}{Linear Disciminant Analysis}, is a machine learning technique often used in combination with CSP\cite{ErrorPotentials,svmldacomp,currTrends}. LDA looks for a projection of the data where the data is linearly separable, as shown in Figure \ref{lda}. Looking at the coefficients of the LDA model, one can again determine the importance of the different features.

\mijnfiguur{width=0.7\textwidth}{lda}{LDA finds a projection of the data where the separation of the data is clear.}

%TODO langer?
\subsubsection{Principal Component Analysis}
Principal Component Analysis (PCA) \nomenclature{PCA}{Principal Component Analysis} is a technique to do dimension reduction. Intuitively, PCA can be seen as fitting an n-dimensional ellipsoid to the data. The Principal components are then the axes of the ellipsoid. Less variation in one direction, corresponds to a smaller axis, removing that axis, will only remove a small fraction of the information. This is shown in Figure \ref{ellipsoid}, where the ellipsoid covers a three dimensional features space. The ellipsoid has three axes: a,b and c. Intuitively, one can see that there is more variation (information) in the c ans b direction, while the a axis is relatively small.

\mijnfiguur{width=0.7\textwidth}{ellipsoid}{Suppose a three-dimensional feature space, where all points lie in the ellipsoid in the left.}

Remove the a axis by projecting the data on the plane given by vectors b and c, and one will end up with a two dimensional projection of the data in the form of an ellipse. This would be the black plane in Figure \ref{ellipsoid}. This process can be repeated for higher dimensional features spaces. In other words, PCA will thus, without going into too much detail, start with an n-dimensional ellipsoid and iteratively remove the smallest axis in each iteration until the desired number of dimensions is obtained. Note that the ellipsoid should be adjusted in each step.
The major disadvantage of PCA is that the algorithm is unsupervised, meaning that it does not look at the corresponding labels of the given samples, as a result valuable information might be thrown out.

\subsection{Advanced methods}
These methods are more advanced feature selection methods found in the literature.

\subsubsection{RF feature selection}
One advanced method for feature selection is the two-step method using random forest, described in \citep{rfPaper}. The paper states that there are two possible motivations for feature selection. The first motivation is to do interpretation, find out which features are important and use them for research. In the context of BCI, feature interpretation could help neuroscientist find out which parts of the brain are affected by an emotion, for example. The second motivation is to improve machine learning techniques, having fewer features will not only speed up training and prediction times, it also reduces the complexity, which often has a good influence on the generalisation property of a machine learning algorithm. Additionally in the context of BCI research and EEG data gathering, using fewer electrodes means less preprocessing time; mounting 32 electrodes to the brain of a subject is a time consuming task.

\npar

The selection procedure itself consists of two steps, in the first step data is fitted to a random forest and the importance values for each feature are determined, by taking the average and standard deviation of the importances over all trees. All features are then ranked based on their importance ranking, before features with small importance are cancelled. 

\npar

Then depending on the motivation of feature selection, a second step is performed. For feature interpretation the second steps by fitting a random forest with a single feature. The OOB is then averages over multiple runs. the runs are inserted because a random forest has an element of randomness, fitting the same data twice to a random forest, will not give you the same random forest. The average OOB score and its standard deviation is then used to determine an initial OOB score.

\begin{center}
$OOB_{init} = AVG(OOB) - STD(OOB)$
\end{center}

The standard deviation is used to avoid noisy results, a result is only regarded as better, when there is statistical prove. Next features are added iteratively, when a larger features set has a better average OOB score (taking the standard deviation into account), the feature set is replaced by the larger feature set.

\npar

The other second step is used for prediction, here the algorithm starts similarly, by determining an initial average OOB score and standard deviation. The idea behind the standard deviation is the same as with the interpretation step, noise removal.

\begin{center}
$OOB_{init} = AVG(OOB) - STD(OOB)$
\end{center}

The next part is different, now a feature is introduced in each iteration. When the average OOB score of the feature is better, the feature is added to the features set, otherwise it is neglected. This is a greedy forward selection algorithm, once a feature is selected it remains selected. The difference between step two interpretation and step two prediction is that here single features are added to the feature set, while step two-interpretation always takes the feature set containing all features with higher importance than the lastly added feature. step two prediction on the other hand is able to select a distinct set of features out of the results from step one.

\npar

In the end the paper notes several observations, the step two-prediction method provides better OOB scores using fewer features. Additionally they mention that highly correlated features might confuse the algorithm, as correlated features have lower importances.