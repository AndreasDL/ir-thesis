\chapter{Methods}
{\samenvatting This section will first explain what feature selection means and why it is used, before giving a detailed overview of the different methods.}

\section{Feature selection}
Feature selection is the process of selecting good features from a set of feature. The need for this is twofold: first by reducing the number of features, you can protect yourself against overfitting. This is important when the dataset is limited. Second, reducing the number of features can speedup the learning process of a learning algorithm as fewer parameters need to be optimized. Additionally, in the context of research and this thesis, looking at which features are important gives insight in the problem. In the context of this thesis, knowing what feature are relevant can help neuroscientists understand the working of the brain better.

\subsection{Pearson Correlation}
The Pearson correlation coefficient measures the linear relationship between two variables. The output is a value r, that lies between -1 and 1, corresponding to perfect negative correlation and perfect positive correlation respectively. A correlation value of 0 means that there is no correlation.

\npar

More formally\citep{corrPaper}, the Pearson product-moment coefficient of correlation, r between variables $X_i$ and $Y_i$ of datasets $X$ and $Y$ is defined as:


\begin{center}
$r = \frac{SS_{xy}}{\sqrt{SS_{xx}SS_{yy}}}$
\end{center}
with
\begin{center}
$SS_{xy} = \sum\limits_i (X_i-\tilde{X})(Y_i-\tilde{Y})$
\end{center}
and
\begin{center}
$SS_{xx} = \sum\limits_i (X_i-\tilde{X})^2$ \\
$SS_{yy} = \sum\limits_i (y_i-\tilde{Y})^2$
\end{center}

\npar

The Pearson correlation coefficient is fast and simple to calculate, but has some major shortcomings. First off, it can only see linear relation ships and will not see the correlation between a value $x$ and $x^2$.

\npar

In the context of this thesis, whether the correlation is positive or negative is not important; a learning algorithm needs features that have significant correlation. As a result the absolute value of the r value is reported as this allows for faster comparison of correlations.

\subsection{Mutual Information}
Mutual information is a more robust option for correlation estimation. The mutual information, I, of two variables $X$ and $Y$ is defined as \citep{mutPaper}:
\begin{center}
$I(X,Y) = \sum\limits_{y\in Y} \sum\limits_{x\in X} p(x,y)log(\frac{p(x,y)}{^(x)p(y)}$
\end{center}

\npar

Using the mutual information directly for feature ranking might be inconvenient for two reasons. Firstly, it doesn't lie in a fixed range and it is hard to compute for continuous variables. One solution for this problem is to normalize the mutual information scores, so that the results lies between 0 and 1.

The normalized mutual information, NMI of variables X and Y is given by:
\begin{center}
$NMI(X,Y) = \frac{H(X) + H(Y)}{H(X,Y)}$
\end{center}
With $H(X)$ and $H(Y)$ being the Shannon entropy of variable X and variable Y, defined as:
\begin{center}
$H(X) = \sum\limits_{i\in X} p_ilog(\frac{1}{p_i}) = - \sum\limits_i p_ilog(p_i)$\\
$H(Y) = \sum\limits_{i\in Y} p_ilog(\frac{1}{p_i}) = - \sum\limits_i p_ilog(p_i)$
\end{center}
\npar

%\subsection{ANOVA}
%TODO

\subsection{Distance Correlation}
Distance correlation is a relatively new technique that is designed explicitly to address shortcomings of Pearson correlation. A Pearson correlation coefficient of zero implies that the variables might be independent, but as mentioned before, does not guarantee this. 

\npar

The distance covariance is defined as\citep{distPaper}:
\begin{center}
$dCov^2(X,Y) = \frac{1}{n^2}\sum_limits_{k,l=1}^{n} A_{k,l}B_{k,l}$
\end{center}
With A, B being simple linear functions of the pairwise distances between sample elements. This metric is a covariance metric, which means that it is not normalized. The distance correlation is the normalized version of the distance covariance and is defined as:

\begin{center}
$dCor(X,Y) = \frac{dCov(X,Y)}{\sqrt{dVar(X)dVar(Y)}}$
\end{center}
With $dCov(X,Y)$ being the aforementioned distance covariance, $dVar(X)$ and $dVar(Y)$ are the distance standard deviations. 

\npar

The distance correlation has the disadvantage that is much slower than mutual information or Pearson correlation, but in return, the distance correlation is able to detect more complex relationships between two variables.

\subsection{Linear Regression}
Another way of finding relevant features is to use model based ranking. In model based ranking an arbitrary machine learning method is used to build a model. Looking at the coefficients of the trained model, the importance is determined by its own coefficient. High coefficients mean that the feature has a lot of influence on the output, while low coefficients correspond to less important features.

\npar

The first method to use is simple linear regression. This method tries to find a linear combination of features that produces the output value. Linear regression can achieve good results given that the data doesn't contain a lot of noise and the features are (relatively) independent. When the set of features contains correlated features, the model becomes unstable. As a result, small changes in input data might lead to huge differences in output coefficients. for example assume the 'real output' is given by $Y = X_1 + X_2$ and the dataset contains output in the form of $Y = X_1 + X_2 + \epsilon$ with $\epsilon$ being some random noise. Further more assume that $X_1$ and $X_2$ are linearly correlated, meaning that $X_1 \approx X_2$. The suspected output of the model should be $Y = X_1 + X_2$, but since noise is added the algorithm might end up with arbitrary combinations of $X_1$ and $X_2$, e.g. $Y = -X_1 + 3X_2$ and rate one feature much higher than another one, while in reality they are of equal importance. This is due to the noise; by maximizing the performance, the algorithm will minimize the influence of noise on the output, which result in unstable behaviour when sufficient correlated features are present. 

\clearpage

\subsection{Lasso Regression}
Lasso regression uses L1 regularization, that adds a penalty $\alpha\sum\limits_{i=1}^{n} |w_i|$ to the loss function. the result is that the coefficients of weak features are forced to zero, as each non-zero feature adds to the penalty. This form is regularization is thus quite aggressive, it removes weak features completely. The problem with this is, again, stability; coefficient can vary significantly even for small changes in training data, when there are correlated features.

\subsection{Ridge Regression}
Ridge regression uses L2 regularization, which add a L2 norm penalty to the loss function, given by $\alpha\sum\limits_{i=1}^{n} w_i^2$. Where the L1 norm forces the coefficients to zero, the L2 regularization forces the coefficients to be spread out more equally. The result is that correlated features tend to get similar coefficients, as this minimizes the loss function, which in turn results in a more stable model. 

\subsection{SVM}
Just like Regression uses linear regression to get coefficients, it is also possible to use SVM for feature importance estimation.

\subsection{Random Forests}
Random forests (RF) \nomenclature{RF}{Random Forests} is a efficient learning algorithm based on model bagging and aggregation ideas\citep{rfPaper}. The Random forests work by creating different decision trees. On their own, decision trees are very prone to overfitting. Random forests solve this problem by creating an aggregation of trees. 

\npar

Additionally, some randomness is included, each tree looks at a random subset of the samples and a random subset of the features. This principle is shown in Figure \ref{RF}. This random subset of samples is called the bootstrap sample and is selected out of N samples, by picking N times a sample, with replacement. This results, on average, in 2/3 of the samples being selected (with some doubles). The other 1/3 of the samples are then used as out of bag (oob) \nomenclature{oob}{out of bag} set. Averaging the performance of each tree on the out of bag set, offers an indication of the generalisation of the random forest.

\mijnfiguur{width=0.9\textwidth}{RF}{The structure of a random forest, found at \citep{rfPic}}

\npar

To understand which features are good, one needs to understand the internal workings of a decision tree. Suppose the following example\footnote{This example is based extensively on this youtube video: https://www.youtube.com/watch?v=eKD5gxPPeY0}, where one tries to find an algorithm to predicted whether or not a person will play tennis on a given day. Suppose the training data is given by Table \ref{decisionTreeTable} and a prediction for the $15^{th}$ sample needs to be made.

\begin{table}[H]
\centering
\caption{suppose the following training examples for a decision tree.}
\label{decisionTreeTable}
\begin{tabular}{lllll}
\textbf{Day} & \textbf{Outlook} & \textbf{Humidity} & \textbf{Wind} & \textbf{Play tennis} \\
\textbf{1}   & sunny            & high              & weak          & no                   \\
\textbf{2}   & sunny            & high              & strong        & no                   \\
\textbf{3}   & overcast         & high              & weak          & yes                  \\
\textbf{4}   & rain             & high              & weak          & yes                  \\
\textbf{5}   & rain             & normal            & weak          & yes                  \\
\textbf{6}   & rain             & normal            & strong        & no                   \\
\textbf{7}   & overcast         & normal            & strong        & yes                  \\
\textbf{8}   & sunny            & high              & weak          & no                   \\
\textbf{9}   & sunny            & normal            & weak          & yes                  \\
\textbf{10}  & rain             & normal            & weak          & yes                  \\
\textbf{11}  & sunny            & normal            & strong        & yes                  \\
\textbf{12}  & overcast         & high              & strong        & yes                  \\
\textbf{13}  & overcast         & normal            & weak          & yes                  \\
\textbf{14}  & rain             & high              & strong        & no                   \\
             &                  &                   &               &                      \\
\textbf{15}  & rain             & high              & weak          & ?                   
\end{tabular}
\end{table}

\npar
A decision tree will take a feature and split the data based on the possible outcomes of this feature. In case the features are continuous values, ranges are selected. In some cases the leafs will be pure, like the leaves displayed in  green in Figure \ref{decisionTree}. All examples in here have the same output. In case the leave is not pure, then another split is needed. Note that not all random forests split until all leaves are pure; random forest can be limited in depth, in that case the output is chose by a majority voting of the samples.

\mijnfiguur{width=0.6\textwidth}{decisionTree}{A decision tree for the data in Table \ref{decisionTreeTable}}

Once the tree is constructed it becomes clear that the predicted output of sample 15 is yes. This is obtained simply by following the tree branches. Even though the features are selected at random, they have influence on the accuracy. Good features will reduce the impurity significantly, thus the impurity reductions are a good indication for how important a feature is.

\npar

Since the importance is averaged over different nodes and different trees, it is also capable of detecting combinations of features that work well. One feature may not be important on its own, but might be a very good feature when combined with other features. Suppose the following example in Table \ref{featPair}:

\begin{table}[H]
\centering
\label{featPair}
\begin{tabular}{lll}
\textbf{label} & \textbf{feature A} & \textbf{feature B} \\
\textbf{Happy} & +                  & +                  \\
\textbf{Happy} & -                  & -                  \\
\textbf{Sad}   & -                  & +                  \\
\textbf{Sad}   & +                  & -                 
\end{tabular}
\caption{Some feature are not significant on its own, but a might be part of a combination of features.}
\end{table}

It is clear that feature A and B are very important when it comes to predicting whether or not a person is happy or sad. When both features have the same sign, the person is happy, otherwise he is not. This problem occurs in many simple selection methods. 

\npar

This problem does not occur for random forest though, as combinations of features are also 'tested' in the sense that a tree might split on them in different stages. Once the combination of features occurs randomly in a decision tree, the impurity will drop significantly, which will result in higher importance rankings.

%svm
%rf
%PCA ?

%Rf werkt beter omdat het beter combinaties van features kan inschatten!

%Advanced RF method + paper