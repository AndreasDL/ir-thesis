@article{ExtendedPaper,
author = {Danny Oude Bos},
title = {EEG-based emotion recognition},
year = {2007},
}

@article{CompRecognizeEmotion,
author = {R. W. Picard and J. Klein},
title = {Computers that recognise and respond to user emotion: theoretical and practical applications},
journal = {},
year = {2002},
	volume = {Interacting with computers},
	number = {14},
	pages = {141-169},
	note = {},
	month = {}
}

@article{EmotionRelativePower,
author = {Kwang-Eun Ko and Hyun-Chang and Kwee-Bo Sim},
title = {Emotion Recognition using EEG signals with relative power values and bayesian network},
journal = {International Journal of Control, Automation, and Systems},
year = {2009},
}


@ONLINE{EmotionSite,
author = {Mensia Technologies},
title = {Emotions by Mensia},
month = September,
year = {2015},
url = {http://www.mensiatech.com/emotions-mensia/}
}

@article{RealTimeEEGEmotion,
author = {Yisi Lio and Olga Sourina and Minh Khoa Nguyen},
title = {Real-time EEG based human emotion recognition and visualization},
year = {2010}
}

@book{TenTwentyManual,
author = {Trans Cranial Technologies},
title = {10/20 System Positioning manual},
publisher = {Fortis Tower},
year = {2012}
}

@ONLINE{1020Site,
author = {unknown},
title = {Electrode placement},
year = {2015},
url = {http://aibolita.com/nervous-diseases/3647-electrode-placement.html}
}

@article{MonoBiPolar,
author = {Yuan Yang and Sylvain Chevallier and Joe Wiart and Isabelle Bloch},
title = {Time-Frequency optimization for discrimination between imagination of right and left hand movements based on two bipolar electroencephalography channels},
journal = {EURASIP journal on Advances in Signal Processing},
year = {2014},
	volume = {2014},
	number = {38}
}

@ONLINE{DataSets,
author = {University of Florida},
title = {Media Core},
year = {2015},
url = {http://csea.phhp.ufl.edu/media.html}
}

@ONLINE{WavesSite,
author = {Brainworks},
title = {What are brainwaves?},
year = {2015},
url = {http://www.brainworksneurotherapy.com/what-are-brainwaves}
}

@article{EEGDatasets,
author = {Yisi Lio and Olga Sourina},
title = {EEG Databases for Emotion Recognition},
journal = {International Conference on Cyberworlds},
year = {2013}
}

@mastersthesis{PaperThibault,
author = {Thibault Verhoeven},
title = {Brain-Computer Interfaces with Machine Learning: an improved Paradigm for the P300 Speller},
school = {Ghent University},
year = {2013},
	type = {Afstudeerwerk},
	address = {},
	month = {June},
	note = {}
}

@article{ComparisonClassifications,
author = {Nikolay V. Manyakov and Nikolay Chumerin and Adrien Combaz and Marc M. Van Hulle},
title = {Comparison of classification methods for P300 brain computer interface on diabled subjects},
journal = {Computational intelligence and neuroscience},
year = {2011},
}

@article{HowManyPeople,
author = {Christoph Guger and Shahab Daban and Eric Sellers and Clemens Holzner and Gunther Krausz and Roberta Carabalona and Furio Gramatica and Guenter Edlinger},
title = {How many people are able to control a P300 based brain computer interface BCI?},
journal = {Neuroscience Letters},
year = {2009},
volume = {462},
}

@ONLINE{P300Scheme,
author = {G-Tec medical engineering},
title = {P300 speller tested with several 100 persons in 2010},
year = {2015},
url = {http://www.gtec.at/News-Events/Newsletter/Newsletter-December-2010-Volume-302/articles/P300-speller-tested-with-several-100-persons-in-2010}
}

@mastersthesis{LangModel,
author = {Hannes Verschore},
title = {A Brain-Computer Interface combined with a language model: the requirements and benefits of a P300 speller},
school = {Ghent University},
year = {2012},
	type = {Afstudeerwerk},
	address = {},
	month = {June},
	note = {}
}

@article{ClassTechniqueComp,
  author={Dean J Krusienski and Eric W Sellers and Fran{\c c}ois Cabestaing and Sabri Bayoudh and Dennis J McFarland and Theresa M
Vaughan and Jonathan R Wolpaw},
  title={A comparison of classification techniques for the P300 Speller},
  journal={Journal of Neural Engineering},
  volume={3},
  number={4},
  pages={299},
  url={http://stacks.iop.org/1741-2552/3/i=4/a=007},
  year={2006},
  abstract={This study assesses the relative performance characteristics of five established classification techniques on data collected using the P300 Speller paradigm, originally described by Farwell and Donchin (1988 Electroenceph. Clin. Neurophysiol. 70 510). Four linear methods: Pearson's correlation method (PCM), Fisher's linear discriminant (FLD), stepwise linear discriminant analysis (SWLDA) and a linear support vector machine (LSVM); and one nonlinear method: Gaussian kernel support vector machine (GSVM), are compared for classifying offline data from eight users. The relative performance of the classifiers is evaluated, along with the practical concerns regarding the implementation of the respective methods. The results indicate that while all methods attained acceptable performance levels, SWLDA and FLD provide the best overall performance and implementation characteristics for practical classification of P300 Speller data.}
}


@article{TactileP300,
author = {Anne-Marie Brouwer and Jan B.F. van Erp},
title = {A tactile P300 brain-computer interface},
journal = {Frontiers in Neuroscience},
year = {2010},
	volume = {4},
	number = {19},
	note = {doi:10.3389/fnins.2010.00019},
	month = {May}
}

@article{AuditoryP300,
author = {Johannes H{\"o}hne and Martijn Schreuder and Benjamin Blankertz and Michael Tangermann},
title = {Tow-dimensional auditory P300 speller with predictive text system},
journal = {IEEE EMBS},
year = {2010},
}

@mastersthesis{ErrorPotentials,
author = {A{\"a}ron Coone},
title = {A study on different preprocessing and machine learning techniques for the detection of error-potentials in Brain-Computer Interfaces},
school = {Ghent university},
year = {2011},
	type = {Afstudeerwerk},
	address = {},
	month = {June},
	note = {}
}

@INPROCEEDINGS{P300T9, 
author={Akram, F. and Hee-Sok Han and Hyun Jae Jeon and Kyungmo Park and Seung-Hun Park and Jinsung Cho and Tae-Seong Kim}, 
booktitle={Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual International Conference of the IEEE}, 
title={An efficient words typing P300-BCI system using a modified T9 interface and random forest classifier}, 
year={2013}, 
pages={2251-2254}, 
abstract={The conventional P300-based character spelling BCI system consists of a character presentation paradigm and a classification system. In this paper, we propose modifications to both in order to increase the word typing speed and accuracy. In the paradigm part, we have modified the T9 (Text on Nine keys) interface which is similar to the keypad of mobile phones being used for text messaging. Then we have integrated a custom-built dictionary to give word suggestions to a user while typing. The user can select one out of the given suggestions to complete word typing. Our proposed paradigms significantly reduce the word typing time and make words typing more convenient by typing complete words with only few initial character spellings. In the classification part we have adopted a Random Forest (RF) classifier. The RF improves classification accuracy by combining multiple decision trees. We conducted experiments with five subjects using the proposed BCI system. Our results demonstrate that our system increases typing speed significantly: our proposed system took an average time of 1.83 minutes per word, while typing ten random words, whereas the conventional spelling required 3.35 minutes for the same words under the same conditions, decreasing the typing time by 45.37%.}, 
keywords={brain-computer interfaces;decision trees;electroencephalography;character presentation paradigm;custom-built dictionary;decision trees;modified T9 interface;random forest classifier;text on nine keys;word typing P300-BCI system;Accuracy;Brain-computer interfaces;Dictionaries;Electroencephalography;Neurophysiology;Radio frequency;Support vector machines}, 
doi={10.1109/EMBC.2013.6609985}, 
ISSN={1557-170X}, 
month={July},}

@article{AuditoryP300Effect,
title = "Effects of facial affect recognition on the auditory \{P300\} in healthy subjects ",
journal = "Neuroscience Research ",
volume = "41",
number = "1",
pages = "89 - 95",
year = "2001",
note = "",
issn = "0168-0102",
doi = "http://dx.doi.org/10.1016/S0168-0102(01)00248-6",
url = "http://www.sciencedirect.com/science/article/pii/S0168010201002486",
author = "Yoshifumi Morita and Kiichiro Morita and Masashi Yamamoto and Yoshifumi Waseda and Hisao Maeda",
keywords = "Facial affect recognition",
keywords = "Event-related potentials",
keywords = "Gender ",
abstract = "We examined the effects of facial affect recognition on auditory \{ERP\} using facial drawings depicting sadness, no emotion, pleasure and anger. Auditory \{ERP\} were recorded using an oddball paradigm in 13 women and 13 men while pictures and test tones were presented. \{P300\} peak amplitude, area, and latency, and also subject's reaction time, were evaluated. The face showing pleasure resulted in the smallest \{P300\} peak amplitude. Amplitudes were successively greater with anger, sadness, and no emotion. The \{P300\} area showed facial affect effects resembling effects on peak amplitude. However, facial expression influenced \{P300\} latency in different patterns suggesting the involvement of independent mechanisms. The reproducibility between sessions of the \{P300\} measurements was tested. Both the \{P300\} amplitude and area were largest when viewing neutral pictures, but smallest when viewing pleasant pictures in both sessions. While amplitude and the area of \{P300\} were significantly larger in women than in men, gender was a less potent modifier of the influence of facial expression on \{P300\} parameters. Reduced \{P300\} amplitude and area apparently reflected an inhibitory effect of attention by emotion from facial expressions, especially for pleasure. "
}

@article{StimulusChangesinP300,
    author = {Jin, Jing AND Allison, Brendan Z. AND Kaufmann, Tobias AND Kübler, Andrea AND Zhang, Yu AND Wang, Xingyu AND Cichocki, Andrzej},
    journal = {PLoS ONE},
    publisher = {Public Library of Science},
    title = {The Changing Face of P300 BCIs: A Comparison of Stimulus Changes in a P300 BCI Involving Faces, Emotion, and Movement},
    year = {2012},
    month = {11},
    volume = {7},
    url = {http://dx.doi.org/10.1371%2Fjournal.pone.0049688},
    pages = {e49688},
    abstract = {<sec><title>Background</title><p>One of the most common types of brain-computer interfaces (BCIs) is called a P300 BCI, since it relies on the P300 and other event-related potentials (ERPs). In the canonical P300 BCI approach, items on a monitor flash briefly to elicit the necessary ERPs. Very recent work has shown that this approach may yield lower performance than alternate paradigms in which the items do not flash but instead change in other ways, such as moving, changing colour or changing to characters overlaid with faces.</p></sec><sec><title>Methodology/Principal Findings</title><p>The present study sought to extend this research direction by parametrically comparing different ways to change items in a P300 BCI. Healthy subjects used a P300 BCI across six different conditions. Three conditions were similar to our prior work, providing the first direct comparison of characters flashing, moving, and changing to faces. Three new conditions also explored facial motion and emotional expression. The six conditions were compared across objective measures such as classification accuracy and bit rate as well as subjective measures such as perceived difficulty. In line with recent studies, our results indicated that the character flash condition resulted in the lowest accuracy and bit rate. All four face conditions (mean accuracy &gt;91%) yielded significantly better performance than the flash condition (mean accuracy = 75%).</p></sec><sec><title>Conclusions/Significance</title><p>Objective results reaffirmed that the face paradigm is superior to the canonical flash approach that has dominated P300 BCIs for over 20 years. The subjective reports indicated that the conditions that yielded better performance were not considered especially burdensome. Therefore, although further work is needed to identify which face paradigm is best, it is clear that the canonical flash approach should be replaced with a face paradigm when aiming at increasing bit rate. However, the face paradigm has to be further explored with practical applications particularly with locked-in patients.</p></sec>},
    number = {11},
    doi = {10.1371/journal.pone.0049688}
}        

@ARTICLE{P300TwoParts,
    author = {Nancy K. Squires and Kenneth C. Squires and Steven and A. Hillyard},
    title = {Two varieties of long-latency positive waves evoked by unpredictable auditory stimuli in man},
    journal = {Electroencephalography and Clinical Neurophysiology},
    year = {1975},
    pages = {387--401}
}


@ARTICLE{DEAP, 
author={Koelstra, S. and Muhl, C. and Soleymani, M. and Jong-Seok Lee and Yazdani, A. and Ebrahimi, T. and Pun, T. and Nijholt, A. and Patras, I.}, 
journal={Affective Computing, IEEE Transactions on}, 
title={DEAP: A Database for Emotion Analysis ;Using Physiological Signals}, 
year={2012}, 
volume={3}, 
number={1}, 
pages={18-31}, 
abstract={We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.}, 
keywords={Web sites;electroencephalography;emotion recognition;image classification;information retrieval;multimedia computing;neurophysiology;state estimation;video signal processing;DEAP;EEG signal frequencies;Web site;arousal;decision fusion;dominance;electroencephalogram;emotion analysis;familiarity;frontal face video;human affective states;multimedia content analysis;multimodal data set;music videos;online assessment tool;peripheral physiological signals;single-trial classification;state estimation methods;stimuli selection;video highlight detection;Databases;Electroencephalography;Face;Motion pictures;Multimedia communication;Videos;Visualization;EEG;Emotion classification;affective computing.;pattern classification;physiological signals;signal processing}, 
doi={10.1109/T-AFFC.2011.15}, 
ISSN={1949-3045}, 
month={Jan},}

@article{GivenPaper,
author = {Min-Ki Kim and Miyoung Kim and Eunmi Oh and Sung-Phil Kim},
title = {A Review on the Computational Methods for Emotional State Estimation from the Human EEG},
journal = {Computational and Mathematical Methods in Medicine},
year = {2013},
	volume = {2013},
	number = {573734},
	pages = {13},
}


De lege verwijzingen mogen niet beginnen met het speciale at-teken. 
Kopieer en Plak de template die je nodig hebt en vervang dan de eerste a door een at-teken.

AONLINE{,
author = {},
title = {},
month = ,
year = {},
url = {}
}

Aarticle{,
author = {},
title = {},
journal = {},
year = {},
	volume = {},
	number = {},
	pages = {},
	note = {},
	month = {}
}

Abook{,
author = {},
title = {},
publisher = {},
year = {},
	volume = {},
	series = {},
	month = {},
	edition = {},
	address = {},
	note = {}
}

Ainbook{,
author = {},
title = {},
chapter = {},
pages = {},
publisher = {},
year = {},
	volume = {},
	series = {},
	type = {},
	address = {},
	month = {},
	edition = {},
	note = {}
}

Amastersthesis{,
author = {},
title = {},
school = {},
year = {},
	type = {Afstudeerwerk},
	address = {},
	month = {},
	note = {}
}

Aphdthesis{,
author = {},
title = {},
school = {},
year = {},
	type = {Doctoraatsthesis},
	address = {},
	month = {},
	note = {}
}
Aunpublished{,
author = {},
title = {},
note = {},
	month = {},
	year = {}
}
Ainproceedings{,
author = {},
title = {},
booktitle = {},
year = {},
	editor = {},
	volume = {},
	series = {},
	pages = {},
	address = {},
	month = {},
	organization = {},
	publisher = {},
	note = {}
}
Amanual{,
title = {},
	author = {},
	organization = {},
	address = {},
	edition = {},
	month = {},
	year = {},
	note = {}
}
Amisc{,
	author = {},
	title = {},
	howpublished = {},
	month = {},
	year = {},
	note = {}
}