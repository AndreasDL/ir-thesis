\chapter{Machine Learning}
{\samenvatting }


\section{What is machine learning?}
Since machine learning is a very broad domain, this discussion will be limited to the application of machine learning and machine learning techniques as this is the most relevant part. Machine learning is the science of getting computers to act without being explicitly programmed. Machine learning uses pattern recognition to find patterns or structure in the data. When applied correctly, machine learning algorithms can recognize objects in images, e.g. Optical Character Recognition (OCR)\nomenclature{OCR}{Optical Character Recognition}.

\npar

Quite vague explanation so far, let's illustrate this with a small example. Suppose one has a price list of house that are for sale combined with their total area. Now since it is easy to see that a bigger house will have a higher asking price than a smaller house, one says that the area of the house is correlated with the asking price. Suppose you want to predict how much a certain home is worth, based on their area. This is possible with machine learning, you first learn/teach or train your machine learning algorithm with the aforementioned list. Once this is done you can predict prices of new houses based on the corresponding area. The area of the house is only one property or feature for the algorithm. So far the algorithm can only look at one property and thus is not likely to achieve good results. Looking with more detail at the data, i.e. adding additional features will often improve the performance of an algorithm. For example, a house with 5 bedrooms is more expensive than a house with only 3 bedrooms.

\npar

Machine learning algorithms are responsible for the relation between features and the predicted value. One way to group this algorithms is to look at their produced output. In the asking price examples above, the output is a price, which is a continuous value. When one tries to recognize characters in a picture, the problem becomes a classification problem, as there are only 26 distinct characters. 

\npar

Another way to group the algorithms is based on their training data. In the asking price examples above one gets labelled results; the asking price is given for each area, this is referred to as supervised machine learning. The other possibility is unsupervised machine learning, which often results in finding groups of similar data points (clustering). Not that the combination of supervised and unsupervised data is also possible. Support you have a dataset with 5000 webpages and you want to categorise them in 10 distinct categories, e.g. science, nature, cooking, ... , but you only have the labels for 100 of the 5000 pages. Then you could first cluster the pages in similar groups. As soon as a group contains one labelled page, you can label all the pages in the group as they are similar. This technique is known as semi supervised learning and has the advantage that one can also use unlabelled data. Note that labelled data is usually quite rare; if you had a fast and easy way to label the data then you wouldn't be needing machine learning.

\section{over and underfitting and the relation to high bias and high variance}
%train and test error


\mijnfiguur{width=0.9\textwidth}{overunderfitting}{Overfitting versus underfitting\cite{overunderfitting}.}

Suppose the example in Figure \ref{overunderfitting}, where one tries to find a good function to fit the given data points. Looking at the three proposed functions, one can easily see that the middle figure is the most likely generator function of the red points. 

The figure on the left corresponds to an underfit, where the proposed function is not able to capture sufficient detail of the points. The function is not complex enough to approach the generator function, which is known as high bias. A high bias problem has a high training error, as the function is not able to fit the points correctly, this is visible in Figure \ref{highbias}

\mijnfiguur{width=0.9\textwidth}{highbias}{A high bias function is not complex enough to approach the generator function closely.}

The function on the rights The function on the right corresponds to an overfit; the function 'goes through' each point exactly, but one can see that in between data points the behaviour of the hypothesis function is not logical. This problem is known as a high variance problem, where the train error is close to zero, but the test error is quite dramatic. 

\mijnfiguur{width=0.9\textwidth}{highvariance}{A High variance function is too complex and fits the data point too closely.}

Another way to explain the bias variance tradeoff is by an example. Suppose you have a dart board, as shown in Figure \ref{BiasVariance}. Suppose the situation on the top left corner, this corresponds to a world class player that has perfect aim, and very little variation on his precision. The situation on the left bottom corresponds to a player that has very little variation on his precision, but that is consistently aiming too high, he is biased to hit higher than needed. The pictures on the right side are different, there the person may or may not have a biased aim, but it is clear that he has a lot of variation in the precision of his aim.
\npar
In the context of machine learning, the low bias corresponds to having a hypothesis set that is close to the generator function, which allows you to get quite close. However you still have to pick the right function from that set, which is hard to do if you don't have enough data. If you are not able to take the best solution from the hypothesis set, you have high variance.

\mijnfiguur{width=0.9\textwidth}{BiasVariance}{The bias variance explained using the dartboard example found at \cite{biasvariance}}


\section{Support Vector Machines (SVM)}


\section{Linear Discriminant analysis (LDA)}


\section{Common Spatial Patterns (CSP)}


\section{Random Forests (RF)}

\section{Pearson correlation}
