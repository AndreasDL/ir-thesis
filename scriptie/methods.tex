\chapter{Methods}
{\samenvatting This chapter starts by explaining the used dataset and features. Next some state of the art methods are briefly discussed, to give an idea of similar research. After that the contributions of this thesis are explained. Next different feature selection methods are explained. The last part of this section explains the used approach in this thesis.}

\section{Dataset}
One of the most used datasets in the context of emotion recognition is the Dataset for Emotion Analysis using Physiological Signals (DEAP)\nomenclature{DEAP}{Dataset for Emotion Analysis using Physiological Signals}\cite{DEAP}. This dataset consists of several parts, the first part is a rating of 120 music videos by 14 - 16\footnote{2 persons did not complete all the necessary ratings.} persons. Each video is rated for valence, arousal and dominance on a scale ranging from 1 to 9 using self-assessment manikins (see later). This part of the dataset is not used during this thesis, because it contains no physiological signals.

\npar

The next part of the dataset is the physiological experiment that contains emotional reactions of 32 subjects. The emotional reactions were triggered using music video excerpts. Each subject watched 40 one-minute videos, while several physiological signals were recorded. These physiological signals consist of 32 channel, 512Hz EEG signals combined with peripheral physiological signals like respiration rate, skin temperature, etc. More concretely, this dataset contains following signals:
\begin{table}[H]
\centering
\begin{tabular}{l|ll|l|ll}
\textbf{Channel} & \textbf{Name} & \textbf{Category} & \textbf{Channel} & \textbf{Name}    & \textbf{Category} \\ \hline
\textbf{1}       & Fp1           & EEG               & \textbf{21}      & F8               & EEG               \\
\textbf{2}       & AF3           & EEG               & \textbf{22}      & FC6              & EEG               \\
\textbf{3}       & F3            & EEG               & \textbf{23}      & FC2              & EEG               \\
\textbf{4}       & F7            & EEG               & \textbf{24}      & Cz               & EEG               \\
\textbf{5}       & FC5           & EEG               & \textbf{25}      & C4               & EEG               \\
\textbf{6}       & FC1           & EEG               & \textbf{26}      & T8               & EEG               \\
\textbf{7}       & C3            & EEG               & \textbf{27}      & CP6              & EEG               \\
\textbf{8}       & T7            & EEG               & \textbf{28}      & CP2              & EEG               \\
\textbf{9}       & CP5           & EEG               & \textbf{29}      & P4               & EEG               \\
\textbf{10}      & CP1           & EEG               & \textbf{30}      & P8               & EEG               \\
\textbf{11}      & P3            & EEG               & \textbf{31}      & PO4              & EEG               \\
\textbf{12}      & P7            & EEG               & \textbf{32}      & O2               & EEG               \\
\textbf{13}      & PO3           & EEG               & \textbf{33}      & hEOG             & non-EEG           \\
\textbf{14}      & O1            & EEG               & \textbf{34}      & vEOG             & non-EEG           \\
\textbf{15}      & Oz            & EEG               & \textbf{35}      & zEMG             & non-EEG           \\
\textbf{16}      & Pz            & EEG               & \textbf{36}      & tEMG             & non-EEG           \\
\textbf{17}      & Fp2           & EEG               & \textbf{37}      & GSR              & non-EEG           \\
\textbf{18}      & AF4           & EEG               & \textbf{38}      & respiration belt & non-EEG           \\
\textbf{19}      & Fz            & EEG               & \textbf{39}      & plethysmograph   & non-EEG           \\
\textbf{20}      & F4            & EEG               & \textbf{40}      & skin temperature & non-EEG          
\end{tabular}
\caption{The available signals in the DEAP dataset\label{DEAPSignals}.}
\end{table}

\npar

A preprocessed version of the physiological experiment database is also available. In this version, the EEG recordings were downsampled to 128Hz and noise and EOG artifact removal was performed. A bandpass filter was applied to filter out frequencies below 4Hz and above 40-45Hz. This was done to remove noise, since most muscle and eye artifacts have a frequency around 1.2Hz and artifacts caused by nearby power lines, have a frequency around 50Hz\cite{ExtendedPaper}. This thesis uses the preprocessed version of the DEAP since it is the most practical version to use.

\npar

Additionally facial video for 22 of the 32 subjects was recorded, so research of facial expressions is also possible with this dataset. All videos are rated on 4 scales: arousal, valence, dominance and liking. The liking component indicates how much each person liked each video excerpt. It is important not to confuse the liking component with the valence component, as it inquires information about the participants' tastes, not their feelings. For instance, a person can like a video that triggers angry or sad emotions\footnote{However strong correlations between the liking and valence ratings were observed\citep{DEAP}.}. The liking rates are mentioned here for completeness and are not used in this work as they are not part of the emotion space.

\npar

For assessment of these scales self-assessment manikins (SAM)\nomenclature{SAM}{self-assessment manikins} were used\cite{DEAP}. SAM visualizes the valence, arousal and dominance scales with pictures. Each picture corresponds to a discrete value. The user can click anywhere in between the different figures, which makes the scales continuous. All dimensions are given by a continuous value between 1 and 9.

\npar

The used SAM figures are shown in Figure \ref{SAM}. The first row gives the valence scale, ranging from sad to happy. The second row shows the arousal scale, ranging from bored to excited. The last row represents the different dominance levels. The left figure represents a submissive experience, while the right figure corresponds with a dominant experience.

\mijnfiguur{width=0.5\textwidth}{SAM}{The images used for the SAM\cite{DEAP}.}

\section{Features}
\label{featuresExplained}
Machine learning algorithms require good features to perform well\footnote{There are some exceptions, for instance some types neural networks are capable of 'designing' their own features\citep{nnfeat}. But these algorithms were not used in this thesis.}. In the context of this thesis, good features should be correlated with the subject's emotional state. Two categories of features are observed in this work: EEG features and non-EEG features. Both categories are covered in the following sections.

\subsection{EEG-features}
EEG features are extracted from the electroencephalography measurements from the subject's scalp. From these signals a lot of different signals can be extracted. The power spectral density (PSD) \nomenclature{PSD}{Power Spectral Density} of a signal gives the distribution of the signal's energy in the frequency domain. By calculating the spectral density for different frequency bands of the signal, one can determine how much power of each frequency band is in the signal.

\npar

Differential entropy (DE)\nomenclature{DE}{Differential Entropy} is defined as follows \citep{killyPaper} \\
\begin{center}
$DE_{channel} = - \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi\sigma^2}} exp(\frac{(x-\mu)^2}{2\sigma^2}) log(\frac{1}{\sqrt{2\pi\sigma^2}}) exp(\frac{(x-\mu)^2}{2\sigma^2})dx$
\end{center}
It is proven that the differential entropy of a certain band is equivalent to the logarithmic power spectral density for a fixed length EEG sequence\citep{diffEnt}. This simplifies the calculations significantly.
\begin{center}
$DE_{channel} = log(PSD_{channel})$
\end{center}

\npar

The most used feature for valence recognition is the frontal asymmetry of the alpha power\cite{GivenPaper}. The right hemisphere is generally speaking, more active during negative emotion than the left hemisphere. The left hemisphere is in turn more active during positive emotions\cite{RealTimeEEGEmotion,EEGDatasets,killyPaper}. The asymmetry can be calculated in different ways. First, one can calculate the differential asymmetry (DASM) \nomenclature{DASM}{Differential Asymmetry}, where the left alpha power is subtracted from the right alpha power.

\begin{center}
$DASM = DE_{left} - DE_{right}$
\end{center}

Another way to measure the asymmetry is by division. The Rational Asymmetry (RASM) \nomenclature{RASM}{Rational Asymmetry} does exactly this and is given by: \\

\begin{center}
$RASM = \frac{DE_{left}}{DE_{right}}$
\end{center}

With $DE_{left}$ and $DE_{right}$ being the left and right differential entropy respectively. Another reported feature in literature is the caudality, or the asymmetry in fronto-posterior direction\cite{caudality}. Caudality measures the difference in power between the front and the back of the scalp. This can again be calculated in two ways. The first method is the differential Caudality (DCAU) \nomenclature{DCAU}{Differential Caudality}, defined as: \\

\begin{center}
$DCAU = DE_{front} - DE_{post}$
\end{center}

The second method to determine the Caudality is the Rational Caudality (RCAU) \nomenclature{RCAU}{Rational Caudality}, which is defined as:

\begin{center}
$RCAU = \frac{DE_{front}}{DE_{post}}$
\end{center}

With $DE_{front}$ and $DE_{post}$ being the frontal and posterior power respectively. Arousal is usually determined, by looking at the different frequency bands\citep{ExtendedPaper}. Each frequency and has their own medical interpretation, see \ref{wavebands}. Alpha power corresponds to a more relaxed brain, while Beta power corresponds to a more active brain. The alpha / beta ratio therefore seems a good indicator for the arousal state of a person.

\npar

The Alpha/ Beta ratio is limited to comparing two frequency bands. Other frequently used features are fractions of PSD. These fractions indicate what proportion of power a certain frequency band has. They are defined  for a channel, given by:

\begin{center}
$frac_{band,channel} = \frac{power_{band,channel}}{power_{total,channel}}$
\end{center}

These fractions give insight in the distributions of wavebands at different channel locations.

\subsection{non-EEG features}
The aforementioned EEG features are just one class of physiological features, the DEAP dataset contains several other physiological measurements\citep{DEAP}. For each of these measurements the average, standard deviation, variation, median, minimum and maximum are calculated.

\npar

The Galvanic Skin Response \nomenclature{GSR}{Galvanic Skin Response} uses two electrodes on the middle and index finger of the subjects left hand to measure the skin resistance. The skin resistance is one way to measure the perspiration of a subject. It has been reported that the mean value of the GSR is related to the level of arousal\citep{GSR, DEAP}.

\npar

The respiration belt (RSP)\nomenclature{RSP}{Respiration Belt}, indicates the user's respiration rate. Slow respiration is linked to relaxation (low arousal). Fast and irregular respiration patterns corresponds to anger or fear, both emotions have low valence and high arousal\citep{DEAP}.

\npar

A plethysmograph is a measurement of the volume of blood in the subject's left thumb. This can be used to determine the blood pressure of a subject. Blood pressure offers valuable insight into the emotional state of a person. For instance, stress is known to increase blood pressure\citep{DEAP}.

\npar

The heart rate is not directly available in the DEAP dataset. fortunately, it can be extracted from the plethysmograph, by looking at local minima and maxima\citep{DEAP}. This is visible when looking at the plethysmograph's output, shown in Figure \ref{before}.

\mijnfiguur{width=0.9\textwidth}{before}{The plethysmograph before smoothing.}

The heart rate extraction is done in two steps. In the first step the plethysmograph's output is smoothed to filter out high frequency components. This is done to avoid that noise is selected as a local optima. In the second step the local extrema are located, as shown in Figure \ref{extrema}

\mijnfiguur{width=0.9\textwidth}{extrema}{The local optima in the plethysmograph.}

The combination of a local minimum and maximum correspond to a heart beat\citep{DEAP}. Therefore, the time between two consecutive local minima or maxima correspond to the time between two heart beats, known as the interbeat interval. Getting the average heart rate from the interbeat interval is straight forward. Lastly, the skin temperature of the subject is also available.

\subsection{Overview}

The following table gives an overview of the different features and their amount. The 32 EEG channels can be found in Table \ref{DEAPSignals}. There are 6 frequency bands: Alpha, Beta, Gamma, Delta, Theta and All. All refers to taking the total power of a channel. Note that the fractions only have 5 different frequency bands, as the percentage of all power would always be 100$\%$. 

\clearpage

\begin{table}[]
\centering
\begin{tabular}{lllll}
\textbf{Name}           & \textbf{Type} & \textbf{no. Channels}   & \textbf{no. Frequency bands} & \textbf{Total} \\
\textbf{PSD}            & eeg           & 32                            & 6                         & 192          \\
\textbf{DE}             & eeg           & 32                            & 6                         & 192          \\
\textbf{DASM}           & eeg           & 13                            & 6                         & 78           \\
\textbf{RASM}           & eeg           & 13                            & 6                         & 78           \\
\textbf{DCAU}           & eeg           & 11                            & 6                         & 66           \\
\textbf{RCAU}           & eeg  & 11                            & 6                         & 66           \\
\textbf{Frac}           & eeg           & 32                            & 5                         & 160          \\
\textbf{Alpha / Beta}   & eeg           & 32                            & 1                         & 32           \\
\textbf{EEG Total}      &               &                               &                           & 864          \\
                        &               &                               &                           &              \\
\textbf{Name}           & \textbf{Type} & \textbf{no. Statistics} &                          &              \\
\textbf{HR}             & non-eeg       & 6                             &                           &              \\
\textbf{Plethysmograph} & non-eeg       & 6                             &                           &              \\
\textbf{GSR}            & non-eeg       & 6                             &                           &              \\
\textbf{ST}             & non-eeg       & 6                             &                           &              \\
\textbf{RSP}            & non-eeg       & 6                             &                           &              \\
\textbf{non-EEG Total}  &               & 30                            &                           &              \\
                        &               &                               &                           &              \\
\textbf{Overall Total}  & \textbf{894}  &                               &                           &             
\end{tabular}
\caption{An overview of the different features that were compared in this thesis.\label{featOverviewTable}}
\end{table}

The feature set has a size of 894, which is huge considering that there are only 40 samples for each person. Using this many features in combination with the low sample count, will quickly result in overfitting\citep{prml}. To solve this problem, one can either increase the number of samples or decrease the number of features. Increasing the number of samples is hard. Since EEG data is very personal \citep{DEAP}, several recordings of the same subjects are required. 

\npar

Reducing the feature set in size is another possibility. Two methods exists, dimension reduction and feature selection. The difference between dimensionality reduction and feature selection is that dimensionality reduction methods consider all information in the feature space. Feature selection methods, on the other hand, take a subset of the information\citep{PhytoEm}.

\npar

This problem is even more severe in cross-subject emotion recognition system. Here, it is not possible to simply take a limited subset of features. Physiological signals are very personal by nature \citep{DEAP}. Selecting features that work for one person, might therefore not work well on different persons.

\section{State of the art}
This section will give an overview of similar studies and their conclusions. Some of these studies also did some research on cross-subject emotion recognition. Emotion recognition is still in its infancy\citep{emorecoghard} and subject independent features are hard to find \citep{DEAP}. Therefore, research is aimed more towards person specific emotion recognition systems.

\subsection{DEAP method}
The first method of emotion recognition is the DEAP method, described in the DEAP paper\citep{DEAP}, the paper that introduces the DEAP dataset used in this thesis. The research found that Valence shows the strongest correlations with the EEG signals. Additionally the study found correlations in all frequency bands, with an increase in power for the lower range wavebands for an increase in valence. These effects occur in the occipital regions of the brain, above the visual cortices. This might indicate that the subject is focussing on a pleasurable sound. A central decrease in beta power was observed together with a occipital and right temporal increase in power for positive emotions. The research conclude that these observed correlations concur with other neurological studies. The absolute value of the correlations are seldom bigger than $0.1$ for a cross person setting. This indicates that cross person emotion recognition is a non trivial problem. The absolute values of the person specific correlations were around $0.5$.

\npar

The DEAP paper also propose their own classification method for person specific emotion classification. They start by performing feature selection using the Fisher's linear discriminant for feature selection. The Fisher's linear discriminant is defined as:

\begin{center}
$J(f) = \frac{|\mu_1 - \mu_2|}{\sigma_1^2 + \sigma_2^2}$ \
\end{center}

With $\mu$ and $\sigma$ being the mean and standard deviation of feature f. The Fisher's discriminant was calculated for each feature, before a threshold of 0.3 was applied to filter out irrelevant features. The used classifier was a Naive Bayes classifier, which assumes independence of features. The Naive Bayes classifier is a simple classifier that uses the following equation:

\begin{center}
$G(f_1, ..., f_n) = argmax_c p(C=c) \prod\limits_{i=1}^n p(F_i=f_i|C=c)$ \\
\end{center}

With F being the set of features and C the classes. $p(F_i=f_i|C=c)$ is estimated by assuming Gaussian distributions of features and modelling these from the training set.

\subsection{Stable emotion recognition over time}

EEG patterns are not only subject dependent, they are also dependent on the subjects mood and thus might vary in time\citep{killyPaper}. This work starts by researching different EEG features: PSD, DE, DASM, RASM, DCAU, RCAU\footnote{Note that these features are explained in more detail in Section \ref{featuresExplained}.}. The different features are tested on the DEAP dataset. Afterwards, they develop a new dataset, where subjects have repeated trial sessions with some time in between. This dataset is then used to measure the performance of their time independent, subject specific, emotion recognition system.

\npar

Their machine learning set-up is as follows, first they perform feature extraction of the aforementioned features. Then feature smoothing is done using a Linear Dynamic system (LDS) \nomenclature{LDS}{Linear Dynamic System}, that can be expressed by:
\begin{center}
$x_t = z_t + w_t$\\
$z_t = Az_{t-1} + v_t$
\end{center}
$x_t$ denotes the observed variables or features, while $z_t$ denotes the hidden emotion variables. $A$ is a transformation matrix and $w_t$ is Gaussian noise. The need for a linear dynamic system is supported by the assumption that emotion change gradually over time. The LDS filters out components that are not associated with emotional states.

\npar

The list of features at this point is too big and may contain uncorrelated features that might lead to performance degradation of the classifier. Two methods for this are compared, principal component analysis (PCA) and minimal redundancy maximal relevance (MRMR)\nomenclature{MRMR}{Minimal Redundancy Maximal Relevance}. 

\npar

PCA uses an orthogonal transformation to create a lower dimensional feature space starting from the original higher dimensional feature space. It does so by minimizing the loss of information, i.e. the principal component should have the largest possible variance. 
PCA is explained later in Section \ref{pcaExaplained}.

\npar

PCA cannot preserve original domain information like channel and frequency, therefore the paper also uses the MRMR method. MRMR uses mutual information in combination with maximal dependency criterion and minimal redundancy. The algorithm starts by searching features satisfying:

\begin{center}
$max D(S,c), D=\frac{1}{|S|} {\displaystyle \sum_{x_d \in S}} I(x_d;c)$
\end{center}

Where S is the feature subset to select. When two features are highly correlated, the maximal dependency is not likely to change when one of the correlated features is removed. This is expressed by the minimal redundancy condition.

\begin{center}
$min R(S), R = \frac{1}{|S|^2} {\displaystyle \sum_{x_{di}, x_{dj} \in S}} I(x_{di},x_{dj})$
\end{center}

The two conditions are then combined to from the Maximal Relevance Minimum Redundancy, which can be expressed as:

\begin{center}
$max \varphi(D,R), \varphi=D-R$
\end{center}

Note that incremental search methods exists and are often used in practice. After performing the dimensionality reduction, the samples from the DEAP data set are classified in high / low valence and high/low arousal, giving a total of four classes. All values close to the separation border are removed from the training data, as they might confuse the classifier. 

\npar 

For the classification, three conventional and one newly developed pattern classifiers were compared. k-nearest neighbors (KNN) \nomenclature{KNN}{k-nearest neightbors}, logistic regression (LR)\nomenclature{LR}{Logistic Regression}, Support Vector Machines (SVM) and Graph regularized Extreme Learning Machine (GELM) \nomenclature{GELM}{Graph regularized Extreme Learning Machine}. Extreme Learning Machine (ELM) \nomenclature{ELM}{Extreme Learning Machine} is a single layer feed forward neural network\citep{ELMpaper}. GELM is based on the idea that similar shapes should have similar properties and obtains better results for face recognition \citep{GELMpaper} and as the paper concludes, also for emotion classification.

\npar

The study found then performed a study on the different features and concluded that DE features are the most suitable EEG features, followed by the asymmetry features (RASM, DASM, DCAU and RCAU). The LDS smoothing was also found to be the better feature smoothing method. 

\subsection{EEG-based emotion recognition in music listening}

This study\citep{emorecoghard} uses EEG features to recognize 4 different discrete emotions (joy, anger, sadness, pleasure) induced by music. They compared four different feature sets on 6 different wavebands: RASM and DASM of 12 channelpairs, raw PSD of the 24 channels and PSD of 30 channels (including 6 midline channels). The compared set of wavebands consists of: alpha, beta, gamma, delta, tetha and all wavebands. These features were fed to two different classifiers, one Multilayer perceptron (MLP) \nomenclature{MLP}{Multilayer Perceptron} and an SVM. 

\npar

Their main results were that the DASM features worked better than the RASM features and even better than using the corresponding 24 PSD features. They also did research to person independent EEG features and found that their accuracy remained consistent. Note that while these results sound promising, they were unfortunately not performed on the DEAP dataset. Performance of emotion recognition algorithms is known to vary a lot between datasets\citep{PhytoEm}.

\subsection{Comparing selected methods for feature extraction and classification}

In this comparitive study four distrinct emotions (joy, anger,sadness and pleasure) were classified \citep{PhytoEm}. The emotions were triggered by songs that were selected for each subject. The subjects were instructed to select songs themselves, that trigger memories. These memories should in turn, trigger the desired emotions. The four emotional states were mapped in the valence-arousal model. The used features were typical statistical values of physiological signals (Skin Conductivity (SC)\nomenclature{SC}{Skin conductivity}, Electrocardiogram (ECG), Electromygraphy (EMG) and Respiration rate (RSP). 

\npar 

Several feature selection techniques were compared. The first one is the analysis of Variance (ANOVA) \nomenclature{ANOVA}{Analysis of Variance} where the best D features were taken. Sequential forward selection (SFS) \nomenclature{SFS}{Sequential Forward Selection}, where the algorithm starts with an empty feature set and then introduces a new feature in each iteration. Sequential backward selection (SBS) \nomenclature{SBS}{Sequential Backward Selection} is an alternative, where a feature is removed in each iteration. These feature selection methods were also compared to two dimensionality reduction methods: PCA and Fisher projection. 

\npar

The newly formed feature space was then fed to three different classifiers: K-nearest neighbors, Multilayer perceptron and Linear discriminant function. The results indicated that it is easier to classify arousal than valence. This might indicate that non-EEG features might be features for arousal classification, as this work only contains non-EEG features. SFS in combination with fisher seems to give the best classification performance, closely followed by LDF and ANOVA, a less computationally intensive method.

\npar

The paper also concludes that joy was characterized by a faster heart rate, while sadness was identified by low SC and EMG signals. There was also a higher breathing rate for negative valence emotions. They reported limited similarities for the selected features between subjects.

\subsection{Advanced RF feature selection}
\label{rfmethod}
One advanced method for feature selection is the two-step method using random forest\citep{rfPaper}. There are two possible motivations for feature selection. The first motivation is to do interpretation, find out which features are important and use them for research. In the context of this work, feature interpretation could help neuroscientist find out which parts of the brain are affected by emotion. The second motivation is to improve machine learning techniques. Having fewer features will not only speed up training and prediction times, it also reduces the complexity. Reducing complexity often has a good influence on the generalisation property of a machine learning algorithm\citep{prml,rfPaper}. Additionally in the context of EEG data gathering, using fewer electrodes means less preprocessing time; mounting 32 electrodes to the brain of a subject is a time consuming task.

\npar

The selection procedure itself consists of two steps. In the first step, data is fitted to a random forest and the importance values for each feature are determined. The importance values are then averaged and the standard deviation of the importances over all trees is calculated. All features are then ranked based on their importance ranking. Next features with small importance values are cancelled. 

\npar

Then, depending on the motivation of feature selection, one of two possible second steps is performed. For feature interpretation the second step starts by fitting a random forest using a single feature. The OOB is then averaged over multiple runs. The runs are needed because a random forest has an element of randomness; fitting the same data twice to a random forest, will not give you the same random forest. To get an accurate estimate of the performance of a random forest, fitting the data several times is required. The average OOB score and its standard deviation is then used to determine an initial OOB score.

\begin{center}
$OOB_{init} = AVG(OOB) - STD(OOB)$
\end{center}

The standard deviation is used to avoid noisy results. This means that a result is only regarded as better, when it is better from a statistical point of view. Next features are added iteratively, when a larger features set has a better average OOB score (taking the standard deviation into account), the feature set is replaced by the larger feature set. Note that the whole set of features is always considered; it is not possible to leave a feature out and include the next feature.

\npar

The other possible second step is used for prediction, here the algorithm starts similarly, by determining an initial average OOB score and standard deviation. The idea behind the standard deviation is the same as with the interpretation step, noise removal and stability.

\begin{center}
$OOB_{init} = AVG(OOB) - STD(OOB)$
\end{center}

The next part is different, in each iteration a feature is introduced. When the average OOB score and standard deviation of the feature are better, the feature is added to the feature set, otherwise it is neglected. This is a greedy forward selection algorithm, once a feature is selected, it remains selected. The difference between the interpretation and prediction step is that here single features are added to the feature set, while step two-interpretation always takes the whole feature set, meaning all features before the last feature, as a replacement. The prediction version of step two is able to select a distinct set of features out of the results from step one.

\npar

In the end the paper notes several observations, the step two-prediction method provides better OOB scores using fewer features. Additionally they mention that highly correlated features might confuse the algorithm, as correlated features have lower importances.

\section{Contribution of this thesis}

It is clear from the aforementioned papers, that some research has already been done for emotion recognition. The contribution of this thesis is threefold, first compare a bigger ranger of feature selection methods on a bigger set of physiological features. Second, compare EEG features to non-EEG features to see how much information can be retrieved from the EEG signals compared to the non-EEG signals, which are usually easier to obtain. Third, perform the feature selection methods in a cross-subject setting to see which feature generalise well across subjects. It is also important to note that the feature selection will be performed on the DEAP dataset, so that it can serve as a benchmark. This is important as performance of emotion recognition algorithms based on physiological signals often varies a lot for different datasets\citep{PhytoEm}.

\section{Feature selection methods}
\label{FSSel}
Feature selection is the process of selecting good features from a set of features. The need for this is twofold: first reducing the number of features, is a protection mechanism against overfitting\citep{rfPaper}. This is important when a smaller dataset is used. Second, reducing the number of features can speedup the learning process of a learning algorithm as fewer parameters need to be optimized. Additionally, in the context of research, looking at which features are important might give more insight in how emotion is processed by the brain. For example, knowing what features are relevant can help neuroscientists understand the working of the brain better. There is also a practical use of feature selection, limiting the physiological signals to fewer channels, can help the set-up time. Mounting an EEG cap to a subject is a time consuming process. Using fewer electrodes can make the system more convenient to use as it would save time.

\npar

Several approaches for feature selection exists: filter methods, wrapper methods and embedded methods. What follows is an explanation of how each approach works, combined with the used methods of each approach in this thesis.

\subsection{Filter methods}
Filter feature selection methods use an independent metric or statistical test to filter out features with low importance. The most simple example of to simple look at the correlation between each feature and the output. Afterwards, all features with low correlation can be removed.

\subsubsection{Pearson correlation}
The Pearson correlation coefficient measures the linear relationship between two variables. The output is a value r, that lies between -1 and 1, corresponding to perfect negative correlation and perfect positive correlation respectively. A correlation value of 0 means that there is no correlation.

\npar

More formally\citep{corrPaper}, the Pearson product-moment coefficient of correlation, r between variables $X_i$ and $Y_i$ of datasets $X$ and $Y$ is defined as:

\begin{center}
$r = \frac{SS_{xy}}{\sqrt{SS_{xx}SS_{yy}}}$
\end{center}
with
\begin{center}
$SS_{xy} = \sum\limits_i (X_i-\tilde{X})(Y_i-\tilde{Y})$
\end{center}
and
\begin{center}
$SS_{xx} = \sum\limits_i (X_i-\tilde{X})^2$ \\
$SS_{yy} = \sum\limits_i (y_i-\tilde{Y})^2$
\end{center}

\npar

The Pearson correlation coefficient is fast and simple to calculate, but has some major shortcomings. First off, it can only see linear relationships and will not see the correlation between a value $x$ and $x^2$.

\npar

In the context of this thesis, whether the correlation is positive or negative is not important; a learning algorithm needs features that have a significant correlation. As a result the absolute value of the r value is used as this allows for a more convenient comparison of correlations.

\subsubsection{Normalized mutual information}
Mutual information is a more robust option for correlation estimation. The mutual information, MI, of two variables $X$ and $Y$ is defined as \citep{mutPaper}:
\begin{center}
$MI(X,Y) = \sum\limits_{y\in Y} \sum\limits_{x\in X} p(x,y)log(\frac{p(x,y)}{^(x)p(y)}$
\end{center}

\npar

Using the mutual information directly for feature ranking might be inconvenient because its results does not lie in a fixed range. Fortunately, normalized variants of the mutual information score exists. The normalized mutual information, NMI, of variables X and Y is given by:

\begin{center}
$NMI(X,Y) = \frac{I(X,Y)}{\sqrt{(H(X)H(Y))}}$
\end{center}

With $H(X)$ and $H(Y)$ being the Shannon entropy of variable X and variable Y, defined as:

\begin{center}
$H(X) = \sum\limits_{i\in X} p_ilog(\frac{1}{p_i}) = - \sum\limits_i p_ilog(p_i)$\\
$H(Y) = \sum\limits_{i\in Y} p_ilog(\frac{1}{p_i}) = - \sum\limits_i p_ilog(p_i)$

\end{center}

\clearpage

\subsubsection{Distance correlation}
Distance correlation solves some shortcomings the Pearson correlation has. The Pearson correlation coefficient might give a correlation of zero for dependent variables, as shown in Figure \ref{pearson}.

\mijnfiguur{width=0.7\textwidth}{pearson}{Pearson correlation coefficients for different sets of (x,y) points. Note that many coefficients are zero, while there clearly is some correlation. Source: Wikipedia}

The distance covariance, sometimes referred as the Brownian covariance, addresses this problem\citep{distPaper}. Its main idea is that a good measurement for dependence is the 'distance' between the join distribution $f_{XY}$ and the product of the marginal distributions $f_X$ and $f_Y$ weighted by a weight function $W$. This gives the following theoretical function:

\begin{center}
$dCorr_{X,Y} = W( || f_{XY} - f_Xf_y|| )$
\end{center}

The result is that the distance correlation metric gives very different results, as you can see when comparing the distance correlation outputs in Figure\ref{distcorr} with the Pearson correlation outputs in Figure \ref{pearson}.

\mijnfiguur{width=0.7\textwidth}{distcorr}{Distance correlation coefficients for different sets of (x,y) points. Note the difference with the Pearson correlation coefficients in Figure \ref{pearson}. - Source: Wikipedia}

Without going further into the theory, the distance correlation between two variables X and Y, each with n data points can be calculated as 
follows. 

\npar

First compute all pairwise Euclidean distances for both variables.
\begin{center}
$[D_x]_{j,k} = || X_j - X_k||$ \\
$[D_Y]_{j,k} = || Y_j - Y_k||$ \\
$j,k = 1,2,...,n$\\
\end{center}
The result is two n by n distance matrices $D_x$ and $D_y$. Next, both matrices are centered:
\begin{center}
$S_x = C_nD_xC_n$\\
$S_y = C_nD_yC_n$\\
\end{center}
Finally, the covariance is computed.
\begin{center}
$\nu^2(X,Y) = \frac{1}{n^2} \sum\limits_l \sum\limits_k [S_x]_{k,l}[S_y]_{k,l}$ 
\end{center}

This is the distance covariance, which is not normalized. The distance correlation is the normalized version of the distance covariance, $dCorr$, which is defined by:

\begin{center}
$dCorr(X,Y) = \frac{dCov(X,Y)}{\sqrt{dVar(X)dVar(Y)}}$
\end{center}
With $dCov(X,Y)$ being the aforementioned distance covariance, $dVar(X)$ and $dVar(Y)$ are the distance standard deviations. The distance correlation has the disadvantage that is much slower than mutual information or Pearson correlation, but in return, the distance correlation is able to detect more complex relationships between two variables.

\subsubsection{Analysis of variance}

Analysis of variance (ANOVA) is a statistical test to analyse differences between groups. The idea is that the total variance, found in the samples consists of two parts. The first part is the variance within a single group, the second part is the variance between groups. 

\npar

Suppose you want to test the influence of caffeine on the reaction speed\footnote{This example was based on the following video: https://www.youtube.com/watch?v=ITf4vHhyGpc}. To do so, you take two groups of each 10 persons. The first group has to drink a large cup of coffee, the second group is the control group that only drinks water. Next the reaction times of all persons in both groups are measured. From these results it is possible to calculate the total variance as well as the variance within each group and the variance between the groups. 

\npar

If the variance within each group is much larger than the variance in between the groups, one concludes that the groups are similar. The reaction time is thus dependent on the person and not on the caffeine. However should the variance between the groups be much bigger than the variance within each group, than one concludes that the variance in reaction time is caused by the caffeine and not by personal difference.

\subsection{Wrapper methods}
These methods select features by applying an arbitrary machine learning technique and looking at the coefficients of the features. The idea is that features with high coefficients have more influence on the end results than features with a lower coefficients and are therefore more important. Again absolute values are used, since a perfectly negative correlated variable is as useful as a perfectly positively correlated variable.

\subsubsection{Linear regression}

A first method is simple linear regression, where a linear combination of features is searched that produce a good estimate of the output value. Linear regression can achieve good results when the data does not contain a lot of noise and the features are (relatively) independent. When the set of features contains correlated features, the model becomes unstable. As a result, small changes in input data might lead to huge differences in output coefficients. for example assume the 'real output' is given by $Y = X_1 + X_2$ and the dataset contains output in the form of $Y = X_1 + X_2 + \epsilon$ with $\epsilon$ being some random noise. Further more assume that $X_1$ and $X_2$ are linearly correlated, meaning that $X_1 \approx X_2$. The suspected output of the model should be $Y = X_1 + X_2$, but since noise is added the algorithm might end up with arbitrary combinations of $X_1$ and $X_2$, e.g. $Y = -X_1 + 3X_2$. the result will rate one feature much higher than another one, while in reality they are of equal importance. This is due to the noise. While maximizing the performance, the algorithm will minimize the influence of noise on the output, which results in unstable behaviour.

\subsubsection{SVM}

A Support vector machine (SVM) is a well known and proven method for machine learning. It has been used in several emotion recognition studies. An SVM works in essence by creating a hyperplane that separates two classes. Shown in Figure \ref{SVM1} is a simple line separating the red from the blue balls.

\mijnfiguur{width=0.5\textwidth}{SVM1}{One possible separation border.}

This is one possible solution, but note that an SVM will always search for a decision boundary that maximizes the boundary between the two classes, shown in Figure \ref{SVM2}.

\mijnfiguur{width=0.5\textwidth}{SVM2}{A separation with maximal boundary.}

This all works well, as the balls are separable using a single straight line. This is not always the case though. Shown in Figure \ref{SVM3} is a scenario where it is not possible to separate the red balls from the blue ones using a single straight line.

\mijnfiguur{width=0.5\textwidth}{SVM3}{There exists no possible line that can separate the red balls from the blue ones.} 

A solution for this is to transform the input space to the feature space, where it is possible to separate the balls using a hyperplane, this is shown in Figure \ref{SVM4}. Different transformations are possible. Each transformation corresponds to a different kernel, the component of an SVM that handles the transformation.

\mijnfiguur{width=0.7\textwidth}{SVM4}{Transformation to a new features space where the balls can be separated by a hyperplane.} 

Back in the original feature space the separation boundary might look like Figure \ref{SVM5}.

\mijnfiguur{width=0.5\textwidth}{SVM5}{Separation boundary in the original feature space.} 

\subsubsection{Linear discriminant analysis}
Linear Discriminant Analysis (LDA)\nomenclature{LDA}{Linear Disciminant Analysis}, is a machine learning technique often used in combination with CSP\cite{ErrorPotentials,svmldacomp,currTrends}. LDA looks for a projection of the data where the data is linearly separable, as shown in Figure \ref{lda}. Looking at the coefficients of the LDA model, one can again determine the importance of the different features.

\mijnfiguur{width=0.7\textwidth}{lda}{LDA finds a projection of the data where the separation of the data is clear.}

\subsection{Embedded methods}
Embedded feature selection methods are methods that are build-in for some machine learning algorithms.

\subsubsection{Lasso regression}
\label{lassoregression}
Lasso regression uses L1 regularization, that adds a penalty $\alpha\sum\limits_{i=1}^{n} |w_i|$ to the loss function. the result is that the coefficients of weak features are forced to zero, as each non-zero feature adds to the penalty. This form is regularization is thus quite aggressive, it removes weak features completely and selects the good features. The problem with this is, similar to linear regression, stability. Coefficients can vary significantly, even for small changes in training data, when there are correlated features.

\subsubsection{Ridge regression}
Ridge regression uses L2 regularization, which add a L2 norm penalty to the loss function, given by $\alpha\sum\limits_{i=1}^{n} w_i^2$. Where the L1 norm forces the coefficients to zero, the L2 regularization forces the coefficients to be spread out more equally. The result is that correlated features tend to get similar coefficients, as this minimizes the loss function, which in turn results in a more stable model. The disadvantage of ridge regression is that bad features still have low weights. This means that they still have an influence on the output.

\subsubsection{Random forests}
\label{rfexpl}
A random forest (RF) \nomenclature{RF}{Random Forests} is an efficient learning algorithm based on model bagging and aggregation ideas\citep{rfPaper}. The Random forests work by creating different decision trees. On their own, decision trees are very prone to overfitting. Random forests solve this problem by creating an aggregation of trees. 

\npar

The word random in random forest indicates that some randomness is included. Each tree in a random forest looks at a random subset of the samples and a random subset of the features. This principle is shown in Figure \ref{RF}. This random subset of samples is called the bootstrap sample and is selected out of N samples, by picking N samples with replacement. This results, on average, in 2/3 of the samples being selected (with some doubles). The other 1/3 of the samples are then used as out of bag (oob) \nomenclature{OOB}{Out of Bag} set. Averaging the performance of each tree on the out of bag set, offers an indication of the generalisation of the random forest.

\mijnfiguur{width=0.9\textwidth}{RF}{The structure of a random forest, found at \citep{rfPic}}

To understand which features are good, one needs to understand the internal workings of a decision tree. Suppose the following example\footnote{This example is based extensively on this youtube video: https://www.youtube.com/watch?v=eKD5gxPPeY0}, where one tries to find an algorithm to predicted whether or not a person will play tennis on a given day. Suppose the training data is given by Table \ref{decisionTreeTable} and a prediction for the $15^{th}$ sample needs to be made.

\begin{table}[H]
\centering
\caption{suppose the following training examples for a decision tree.}
\label{decisionTreeTable}
\begin{tabular}{lllll}
\textbf{Day} & \textbf{Outlook} & \textbf{Humidity} & \textbf{Wind} & \textbf{Play tennis} \\
\textbf{1}   & sunny            & high              & weak          & no                   \\
\textbf{2}   & sunny            & high              & strong        & no                   \\
\textbf{3}   & overcast         & high              & weak          & yes                  \\
\textbf{4}   & rain             & high              & weak          & yes                  \\
\textbf{5}   & rain             & normal            & weak          & yes                  \\
\textbf{6}   & rain             & normal            & strong        & no                   \\
\textbf{7}   & overcast         & normal            & strong        & yes                  \\
\textbf{8}   & sunny            & high              & weak          & no                   \\
\textbf{9}   & sunny            & normal            & weak          & yes                  \\
\textbf{10}  & rain             & normal            & weak          & yes                  \\
\textbf{11}  & sunny            & normal            & strong        & yes                  \\
\textbf{12}  & overcast         & high              & strong        & yes                  \\
\textbf{13}  & overcast         & normal            & weak          & yes                  \\
\textbf{14}  & rain             & high              & strong        & no                   \\
             &                  &                   &               &                      \\
\textbf{15}  & rain             & high              & weak          & ?                   
\end{tabular}
\end{table}

A decision tree will take a feature and split the data based on the possible outcomes of this feature. In case the features are continuous values, ranges are selected. In some cases the leafs will be pure, meaning that all samples in this leaf belong to a single class. The pure leaves in Figure \ref{decisionTree} are displayed in green. In case the leave is not pure, another split is needed. Note that not all random forests split until all leaves are pure; random forest can be limited in depth, in that case the output is chose by a majority voting of the samples.

\mijnfiguur{width=0.6\textwidth}{decisionTree}{A decision tree for the data in Table \ref{decisionTreeTable}}

\clearpage

Once the tree is constructed it becomes clear that the predicted output of sample 15 is 'Yes'. This is obtained simply by following the tree branches. Even though the features are selected at random, they have influence on the accuracy. Good features will reduce the impurity significantly, thus the impurity reductions are a good indication for how important a feature is.

\npar

The importance is averaged over different nodes and different trees. As a results, random forest are also capable of detecting combinations of features that work well. One feature may not be important on its own, but might be a very good feature when combined with other features. Suppose the following example in Table \ref{featPair}:

\begin{table}[H]
\centering
\begin{tabular}{lll}
\textbf{label} & \textbf{feature A} & \textbf{feature B} \\
\textbf{Happy} & +                  & +                  \\
\textbf{Happy} & -                  & -                  \\
\textbf{Sad}   & -                  & +                  \\
\textbf{Sad}   & +                  & -                 
\end{tabular}
\caption{Some feature are not significant on its own, but a might be part of a combination of features.\label{featPair}}
\end{table}

It is clear that feature A and B are very important when it comes to predicting whether or not a person is happy or sad. When both features have the same sign, the person is happy, otherwise he is not. Combinations of features are often not found by feature selection methods as they look for correlations between a single feature and the output.

\npar

This problem does not occur for random forest though, as combinations of features are also 'tested' in the sense that a tree might split on them in different stages. Once the combination of features occurs randomly in a decision tree, the impurity will drop significantly, which will result in higher importance rankings.

\subsubsection{Principal Component Analysis}
\label{pcaExaplained}
Principal Component Analysis (PCA) \nomenclature{PCA}{Principal Component Analysis} is a technique to do dimension reduction. Intuitively, PCA can be seen as fitting an n-dimensional ellipsoid to the data. The Principal components are then the axes of the ellipsoid. Less variation in one direction, corresponds to a smaller axis. Removing that axis, will only remove a small fraction of the information, as there is only little variation in that direction. This is shown in Figure \ref{ellipsoid}, where the ellipsoid covers a three dimensional features space. The ellipsoid has three axes: a,b and c. Intuitively, one can see that there is more variation (information) in the c ans b direction, while the a axis is relatively small.

\mijnfiguur{width=0.7\textwidth}{ellipsoid}{Suppose a three-dimensional feature space, where all points lie in the ellipsoid in the left.}

Removing the a axis by projecting the data on the plane given by vectors b and c, will result in a two dimensional projection of the data in the form of an ellipse. This would be the black plane in Figure \ref{ellipsoid}. This process can be repeated for higher dimensional features spaces. In other words, PCA will thus, without going into too much detail, start with an n-dimensional ellipsoid and iteratively remove the smallest axis in each iteration until the desired number of dimensions is obtained. Note that the ellipsoid should be adjusted in each step.

\npar

The major disadvantage of PCA is that the algorithm is unsupervised, meaning that it does not look at the corresponding labels of the given samples. Suppose the difference between two classes was clearly given by looking at the a axis in Figure \ref{ellipsoid}. Applying PCA would, in that case, result in a total loss of all information.