\chapter{Introduction}
{\samenvatting This chapters introduces the masterthesis. It starts by introducing the basic concepts of emotion recognition based on physiological signals, machine learning. Then it explains the problem statement and the goal of the thesis, followed by an overview of the next chapters.}

\section{Emotion recognition}

Human-to-machine communication, where humans communicate with machines or computer agents, is becoming more and more common\citep{hummaccom}. Fully understanding human communication is a complex problem. In addition to verbal communication, non-verbal communication is also used to exchange information\citep{EmotionRelativePower}. To better understand human-to-machine communication, more insight in the non-verbal communication is needed. Emotion recognition is becoming an increasingly important field as a result\citep{RealTimeEEGEmotion}.

\npar 

Emotion recognition is the proces of recognizing a subject's emotional state. In psychology a clear distinction between physiological behavior and the conscious experience of an emotion, called expression\cite{ExtendedPaper} is made. Expression consists of many parts, including facial expression, body language and voice concern\citep{EMSpeech}. Unlike expression, the physiological aspect of an emotion, e.g. heart rate, skin conductance and pupil dilation, is much harder to control. This makes emotion recognition based on physiological signals more robust to social masking\citep{PhytoEm}. Social masking is the process where an individual masks or hides their emotions to conform to social pressure. To really know one's emotions, it seems, one has to research the physiological aspect of the emotion.

\npar

\label{valarrdomspace}
Before emotions can be recognized, an objective class model describing different emotions is needed. A simple way of achieving this is using several discrete emotions, e.g. anger, joy, sad and pleasure. A more convienent model to classify emotions is the bipolar arousal-valence model\cite{ExtendedPaper,RealTimeEEGEmotion}, which places emotions in a two dimensional space. The main advantage of using a continuous multidimensional model, is that all emotions are modelled in its space, even when no particular discrete label can be used to define the current feeling. Figure \ref{ArousalValenceModel} shows the mapping of different emotions for this model. 

\clearpage

The valence-arousal model consists of two dimensions. Arousal indicates how active a person is and ranges from inactive/bored to active/excited. The valence indicates if the emotion is perceived as positive or negative. Even though arousal and valence describe emotions quite well, a third dimension, dominance, can also be added. This third dimension indicates how strong the emotional feeling was and ranges from a weak feeling to an empowered, overwhelming feeling. The dominance component can aid to filter out samples of strong feelings, since feelings with low dominance are less likely to show significant effects.

\mijnfiguur{width=0.45\textwidth}{ArousalValenceModel}{The arousal - valence model maps emotions in a two dimensional plane\citep{ValArrFig}}

\subsection{Physiological signals}

Now that the classification model is defined, the different physiological signals will be explained. As mentioned before, these signals are used to do automatic emotion recognition. Physiological signals can be divided in two subgroups: brain activity and other signals, e.g. heart rate, respiration rate, etc. Different technologies exist to record brain activity. The most convenient method is electroencephalography (EEG)\newacronym{EEG}{EEG}{Electroencephalography}, since it is a non-invasive method. Non-invasive methods, in contrast to invasive methods require no surgery. In case of EEG, they simply measure electrical activity using electrodes placed on the scalp.

\npar

Electrical activity in the brain is generated when an incoming signal arrives in a neuron. This triggers some sodium ions to move inside the cell, which in turn, causes a voltage rise\cite{ExtendedPaper}. When this increase in voltage reaches a threshold, an action potential is triggered in the form of a wave of electrical discharge that travels to neighbouring neurons. When this reaction occurs simultaneously in a lot of neurons, the change in electrical potential becomes significant enough, it is measured by the EEG surface electrodes. EEG can thus only capture synchronized activity of many, many neurons\cite{ExtendedPaper}. This explains why EEG signals have low spatial resolution capabilities. EEG measurements consist of electrical potentials of different channels, measured over time, like shown in Figure \ref{eegexample}.

\mijnfiguur{width=0.9\textwidth}{eegexample}{EEG measurements is a trace electrical potentials of different channels over time.\citep{EEGExampleFig}}

Signals originating from the cortex, close to the skull, are easier to measure, while signals originating deeper in the brain cannot be observed directly. Even for signals originating close to the cortex, EEG is far from precise as the bone between the the cortex and electrodes distorts the signal. Additionally, other artifacts like eye and muscle movement add a lot of noise to the signal. This explains why EEG signals are very noisy by nature. Noise removal techniques are therefor advised\citep{noiseRem}. Note that even though EEG data contains a lot of noise and has a low spatial resolution, it still provides significant insight into the electrical activity of the cortex while offering excellent temporal resolution\cite{GivenPaper}.

\npar

To ensure that experiments are replicable, standards for locations of electrodes have been developed. One of these systems is the 10/20 system, an internationally recognized method to describe the location of scalp electrodes\cite{TenTwentyManual}. The numbers 10 and 20 refer to the distances between the electrodes, which are either 10\% or 20\% of the total front-back or left-right distance of the scalp, this is depicted in Figure \ref{1020ElectrodePlacementSystem}. 

\mijnfiguur{width=0.8\textwidth}{1020ElectrodePlacementSystem}{The electrode placement of a 23 channel system\cite{1020Site}.}

Each site is identified with a symbol that determines the lobe and a number that determines the hemisphere location.
\begin{itemize}
\item \textbf{F:} Frontal
\item \textbf{T:} Temporal
\item \textbf{C:} Central
\item \textbf{P:} Parietal
\item \textbf{O:} Occipital
\end{itemize}
Note that no central lobe exists; the C letter is only used for identification purposes. The letter z indicates that the electrode is placed on the central line. Even numbers are used for the right hemisphere, while odd numbers are used for the left hemisphere. Note that the 10/20 system does not require a fixed number of channels. Some experiments may use a different set of channels, but they all follow the same naming convention. In this work, a 32 channel EEG cap is used. The corresponding electrode locations are shown in Figure \ref{1020labels}

\mijnfiguur{width=0.7\textwidth}{1020labels}{Placement of the 32 electrodes in this thesis.}

In the frequency domain, brain waves are usually split up into different bands\cite{EmotionRelativePower,WavesSite}, with a different medical interpretation for each band. These wavebands \label{wavebands} are:
\begin{enumerate}
\item \textbf{Alpha:} 8-13Hz, indicate how relaxed and/or inactive the brain is.
\item \textbf{Beta:} 13-30HZ, indicate a more active and focused state of mind.
\item \textbf{Gamma:} 30-50Hz, relate to simultaneous processing of information from different brain areas.
\item \textbf{Delta:} 0-4hz, these waves are generated during dreamless sleep and meditation.
\item \textbf{Theta:} 4-8Hz, occurs during dreaming.
\end{enumerate}

\npar

Even though EEG is used in this thesis, alternative methods to measure brain activity exist. What follows is an overview of some of these techniques.
\begin{itemize}

\item Magnetoencephalography (MEG)\newacronym{MEG}{MEG}{magnetoencephalography} use magnetic fields to measure brain activity\citep{meg}. Since MEG is more prone to noise from external magnetic signals, i.e. the earth's magnetic field and electromagnetic communication, a magnetic shielded room is required, making this method very expensive and not mobile.

\item Functional magnetic resonance (fMRI) \newacronym{fMRI}{fMRI}{Functional Magnetic Resonance}\citep{fMRI}: works by detecting changes in blood oxygenation and blood flow. An active area of the brain consumes more oxygen and has an increased blood flow.

\item Computed tomography (CT) \newacronym{CT}{CT}{Computed Tomography}\citep{CT}: uses X-rays to create an image of the brain. 

\item Positron emission tomography (PET) \newacronym{PET}{PET}{Positron Emission Tomography}\citep{PET}: this methods uses trace amounts of short-lived radioactive material. When this material undergoes decay, a positron is emitted that is picked up by a detector.

\item Near infrared spectroscopy (NIRS) \newacronym{NIRS}{NIRS}{Near Infrared Spectroscopy}\citep{NIRS}: an optical technique to measure blood oxygenation in the brain. This technique works by shining light in the near infrared part of the spectrum through the skull and measuring how much remerging light is attenuated.

\end{itemize} 

In addition to brain activity, other physiological signals are also used in this work. The most known signal is the heart rate, which measures the number of contractions per minute. Respiration rate gives to number of breaths a human takes in one minute\citep{DEAP}. Another physiological signal is the galvanic skin response. The galvanic skin response measures the electrical characteristics of the skin\citep{GSR, DEAP}. In addition to the electrical characteristics, the temperature of the skin can also be measured. A plethysmograph is another physiological signal, that measures changes in volume within an organ\citep{DEAP}. A plethysmograph can be used to measure a subject's blood pressure and heart rate.

\section{Machine learning}
Machine learning is the missing link between the physiological signals and the emotion recognition. It is, in short, an input output model, that takes physiological signals and maps them to an emotional state. Machine learning is a very broad domain. As a result, this discussion will be limited to an introduction of the basic machine learning concepts with the focus on the application of machine learning and machine learning techniques used in this thesis. 

\npar

A possible definition for machine learning is: "the science of getting computers to act without being explicitly programmed"\citep{MLDef}. To do so, machine learning uses pattern recognition to find patterns or structure in the data. A simple example of machine learning is the Optical Character Recognition (OCR)\newacronym{OCR}{OCR}{Optical Character Recognition}, where a computer recognises characters in pictures\citep{OCR}. An example of OCR is shown in Figure \ref{OCR}.

\mijnfiguur{width=0.3\textwidth}{OCR}{In optical character recognition, a computer uses machine learning to find characters in an image\cite{OCRFigRef}.}

To further explain how machine learning works, have a look at the following example. Suppose one has a price list of houses that are for sale combined with their total area, shown in Table \ref{mlexampleTable}. Logic sense dictates us that a bigger house will have a higher price than a smaller house. The total area is a characteristic of the house that helps us in determining the price. In the context of machine learning, the characteristic 'total area', will be called a feature as the asking price of a house is correlated to the total area.

\begin{table}[H]
\centering
\caption{Total area of different houses and their corresponding asking prices.\label{mlexampleTable}}
\begin{tabular}{ll}
\textbf{Area of the house ($m^{2}$)} & \textbf{Price ( x 1000 EUR)} \\
70                              & 312                          \\
73                              & 429                          \\
76                              & 174                          \\
79                              & 410                          \\
82                              & 334                          \\
$\vdots$                        & $\vdots$
\end{tabular}
\end{table}

One possible way of predicting the asking price of a house is machine learning. Machine learning works in several steps, first you train the machine learning algorithm with a list of asking prices and the corresponding area of the house. This process is called training or fitting and gives the machine learning component an idea to what price corresponds to a house with a certain area. Once trained, the algorithm's output will look like Figure \ref{mlexample}. The black dots represent the data points from Table\ref{mlexampleTable}. The blue line represents the predicted price for different area. The predicted price is simply defined by the total area of the house multiplied by some weight.

\mijnfiguur{width=0.9\textwidth}{mlexample}{The price of a house is determined by its total area.}

\clearpage

Even though, the blue line looks reasonable, there is sometimes a big difference between the predicted value and the actual value. This is due to the fact that the area of the house is only one feature that determines the price. Other features, like the number of bedrooms or the location of the house, were not taken into consideration. Adding additional features, gives more insight into the data, e.g. a house with 5 bedrooms is more expensive than a house with only 3 bedrooms. Having more features is thus likely to improve the performance of the machine learning algorithm.

\npar

There are many machine learning algorithms. One way to group these algorithms is to look at the produced output. In the asking price examples above, the output is a price, which is (more or less) a continuous value. Machine learning problems that require the output of a continuous value, are called regression problems\citep{prml}. In the OCR example above, a picture of a character is classified as a character. This means that OCR is a classification problem, as there are only a limited number of characters in an alphabet\citep{prml}.

\npar

Another way to group algorithms is based on their training data\citep{prml}. In the asking price examples above, the training data consists of labelled results. Labelled training data corresponds to data where the correct output (in this case the asking price) is given for each input (the area). This type of machine learning is referred to as supervised machine learning\citep{prml}. The alternative is unsupervised machine learning\citep{prml}. Unsupervised learning often results in finding groups of similar data points (clustering), without knowing the actual labels. Note that the combination of supervised and unsupervised data, known as semi-supervised learning, is also possible\citep{semiSup}. Imagine a dataset with 5000 webpages that need to be grouped into 10 distinct categories, e.g. science, nature, cooking, ... . Only 100 of the 5000 pages in the train set are labelled. An approach to solve this problem could be to first cluster the pages in similar groups using unsupervised learning. As soon as a group contains a single labelled page, all pages in the group can be labelled accordingly. This is possible because clustering returns groups of similar samples. Semi supervised learning has the advantage that one can also use unlabelled data, which is often easier and cheaper to obtain, unlike labelled data which is usually quite rare; if there was a fast and easy way to label the data then there would not be a need for machine learning.

\subsection{Over- and underfitting}

Over and underfitting is a common problem in many machine learning projects\citep{prml}. Suppose the example in Figure \ref{overunderfitting}, where one tries to find a good function to fit the given data points. The green line corresponds to the generator function, the 'actual' function that generated the outputs. The blue line is the line represents the machine learning algorithm's fit of the data. Looking at the three proposed functions, one can easily see that the middle figure corresponds to the best fit of the sample points.

\mijnfiguur{width=0.9\textwidth}{overunderfitting}{Overfitting versus underfitting\cite{overunderfittingFig}.}

\npar

The figure on the left corresponds to an underfit, where the proposed function is not able to capture sufficient detail of the points. The function is not complex enough to approach the generator function. As a result the best fit will always contain a relatively big error.

\npar

The function on the right corresponds to an overfit. The function is trying too hard to fit each point exactly, which results in low train error for these data points. However, one can see that the behaviour of the hypothesis function in between data points is not what one would expect. This is the result of using a too complex function to fit the data. As a result the error is very low error for these points.

\npar 

Part of a good machine learning algorithm is finding the right tradeoff between overfitting and underfitting. In case of the aforementioned overfitting, it would be better to lower the performance of the algorithm on the sample datapoints, to gain performance on the 'unseen' datapoints. Different techniques exist to estimate how good an algorithm performs on unseen points. to evaluate how good a designed machine learning algorithm performs on unseen data a part of the sample points is put in a test set. This test set is neglected during training and only used after the training of the algorithm is complete. The performance on the test set will indicate how well the algorithm generalises, since these are all unseen points. It is only the performance of the test set that gives a fair estimation of the performance of a machine learning algorithm. 

\npar

Sometimes it might be important to estimate generalisation during training to prevent overfitting. One way to do this, is cross validation. Cross validation (CV)\newacronym{CV}{CV}{Cross Validation} is a technique that separates the data in N folds, as shown in Figure \ref{CVscheme}. The algorithm is then trained on N-1 blocks and tested on the remaining blocks. This is done N times and the average of the performance is then reported as cross validation error. Note that the test set, displayed in red, is not used during cross validation. The test set is kept completely separate to ensure that a fair estimate of the generalisation is achieved.

\mijnfiguur{width=0.55\textwidth}{CVscheme}{Cross validation}

\subsection{Feature selection}
Feature selection is a technique that aims at selecting the features that perform well, while trying to remove irrelevant features\citep{rfPaper}. The advantages of having a smaller features set are twofold. First having fewer features, will lower the risk of overfitting\citep{rfPaper}. Second, knowing which features are important makes it possible for humans to interpret the machine learning model. In this thesis, knowing which features are relevant might help in gaining insight in the processing of emotion by the brain. 

\npar

There exist several approaches to do feature selection. The first one is to simple use a statistical metric and remove all features with low correlation to the output. Another approach is to look at the weights of a model. When a machine learning model gives a large weight to a feature, then that feature is considered more important than a feature with an assigned weight close to zero. Embedded methods also exist, they rely on the build in feature selection mechanisms of some machine learning algorithms. A more thorough  overview of the different feature selection techniques is given in Section \ref{FSSel}. 

\section{Problem statement}

A lot of different physiological features are reported in the literature. Unfortunately, the literature does not fully agree on a specific set of features nor does it agree on what EEG channels and/or frequency bands are most important for emotion recognition. The features that are reported in different studies are often quite different, as you can see in Table \ref{diffFeat} below.

\begin{table}[H]
\centering
\caption{Six different papers on emotion recognition, six different feature sets\label{diffFeat}.}
\begin{tabular}{ll}
\textbf{study} & \textbf{features used}                         \\
\citep{ref4}     & Alpha and beta power                           \\
\citep{ref7}     & PSD and asymmetry features                     \\
\citep{ref8}     & PSD                                            \\
\citep{ref6}     & discrete wavelet transform of alpha, beta and gamma band \\
\citep{ExtendedPaper}	&	alpha/beta ratio, Fpz beta and alpha band power \\
\citep{killyPaper} & PSD, RCAU, DCAU, DASM, RASM, DE \\
\end{tabular}
\end{table}

\npar

Another related problem with physiological signals is that they are very personal by nature. Features that work well for one person might not work well for another person\citep{DEAP}. Finding a set of features that works well for all persons is hard, but it might make the system more robust against personal differences. 

\npar

The last problem is that is hard to compare the performance of different physiological feature studies, as they do not share the same dataset.


\section{Goal of the thesis}
The first goal is finding relevant features for emotion recognition in a person specific setting. This is already quite challenging as there are fuzzy boundaries and individual variation of emotion\citep{emorecoghard}. To do so, the output of different feature selection methods is compared. In a successful scenario, good features are found. These features could be used by a machine learning algorithm to accurately predict the emotions of one person. Some attention will also be spend on comparing non-EEG and EEG features to see which whether it is useful or not to include EEG and/or non-EEG signals in the emotion recognition.

\npar

The second goal is finding features for emotion recognition in a cross-subject setting. In this setting features should generalise well across different persons, thus the algorithm should be able to recognize emotions from unseen persons. The comparison for non-EEG and EEG features will also be done here. Emotion recognition is harder in a cross-subject setting, since physiological signals are very personal\citep{DEAP}.

\npar 

Both goals are tackled by comparing a large range of different feature selection methods combined with a huge feature set. Additionally, the accuracy on the DEAP, a dataset designed to compare different emotion recognition studies\citep{DEAP} will be reported. This will ensure that the results obtained in this thesis can serve as a benchmark for future research. This is important as performance of emotion recognition algorithms based on physiological signals often varies a lot for different datasets\citep{PhytoEm}.

\npar 

The contents of this thesis are as follows. The next chapter gives an overview of the dataset, features and feature selection methods that are used in this thesis. It also gives an overview of similar state of the art emotion recognition studies.
 
\npar
Chapter 3 and 4 give an overview of the obtained results for person specific and cross-subject emotion recognition respectively. Chapter 5 gives the conclusion of this work combined with an overview of future research applications.